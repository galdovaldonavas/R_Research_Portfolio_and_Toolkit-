---
title: "Analysis of Satisfaction Survey"
output: html_notebook
---

#1. Introduction

This case study explores opportunities to improve customer service, with a focus on increasing the likelihood that customers recommend the company.

The project was conducted in two phases:
1. **Exploratory analysis** to identify key areas for improvement.
2.  **Modeling the impact of customer complaints on recommendation likelihood**, using text analysis and simulation to evaluate how addressing pain points could improve customer service perceptions.

While based on a real-world project, all data, variables, and insights presented here have been simulated to protect confidentiality.

The analysis includes:
- Data simulation and cleaning
- Visualization techniques (e.g., heatmaps for high-dimensional data)
- Descriptive statistics
- Regression modeling (linear, logistic ordinal, and logistic binomial)
- Simulation-based recommendations. 
- Creation of reusable functions for automatizing procedures. 


# 2. Setup

We start by loading the required packages for data manipulation, visualization, modeling, and exporting results.

```{r}
# Data handling
library(readxl)      # Read Excel files
library(openxlsx)    # Write Excel files
library(dplyr)       # Data manipulation
library(tidyr)       # Data reshaping (long/wide formats)

# Visualization
library(ggplot2)     # General plotting
library(coefplot)    # Visualize model coefficients
library(vcd)         # Visualizing categorical data

# Statistical analysis
library(psych)       # Descriptive statistics
library(ordinal)     # Ordinal logistic regression
library(VGAM)        # Alternative ordinal modeling
library(car)         # VIF and regression diagnostics
```


# 3. Data Simulation

## 3.1. Simulating Demographic and Contextual Variables

To create a realistic dataset, we simulate basic **demographic** information (e.g., age, gender) and **contextual variables** related to the customer service experience: country, contact method, and reason for contacting. 

```{r}

# Set seed for reproducibility
set.seed(1000)

# Define dataset size 
data.n <- 40000  # Large enough to allow subgroup analysis despite unbalanced category probabilities

# Initialize dataframe with IDs
data <- data.frame(id = factor(1:data.n))

# Simulate age with a log-normal distribution, capped between 15 and 90
data$age <- round(rlnorm(n = data.n, meanlog = log(40), sdlog = log(1.4)))
data$age[data$age > 90] <- 90
data$age[data$age < 15] <- 15

# Simulate gender
data$gender <- factor(sample(c("man", "woman"),
                             prob = c(0.5, 0.5),
                             replace = TRUE, size = data.n))

# Simulate reason for contacting
data$reason <- factor(sample(c("incorrect_item", "delay", "status", "cancellation", "other"),
                             prob = c(0.20, 0.30, 0.20, 0.10, 0.20),
                             replace = TRUE, size = data.n))

# Simulate contact method
data$method <- factor(sample(c("chat", "web_form", "email", "phone"),
                             prob = c(0.40, 0.10, 0.15, 0.35),
                             replace = TRUE, size = data.n))

# Simulate country (two fictional countries for anonymity)
data$country <- factor(sample(c("Drinkland", "Eatland"),
                              prob = c(0.5, 0.5),
                              replace = TRUE, size = data.n))

```


## 3.2. Simulation of Customer Complaints

We simulate five types of customer complaints: connection issues, slow response, slow resolution, repetition of information, and other unspecified complaints. These are conditioned on demographic and contextual variables as follows:

- **Connection issues**: more likely in Drinkland
- **Slow response**: more frequent in email, and moderately in chat/web_form
- **Slow resolution**: more frequent when "incorrect_item" is handled via chat
- **Repetition**: more likely when the contact reason is "cancellation"
- **Other complaints**: uniformly distributed

```{r}

## Complaints about connection issues
# Base probability of a connection complaint: 10%
prob_connection <- rep(0.10, data.n)
# In Drinkland, probability is doubled
prob_connection[data$country == "Drinkland"] <- prob_connection[data$country == "Drinkland"] * 2
# Ensure probability values remain within [0, 1]
prob_connection [prob_connection > 1] <- 1
prob_connection [prob_connection < 0] <- 0
# Generate binary variable based on probabilities
data$complain.connect <- rbinom(n = data.n, size = 1, prob = prob_connection)
# Check distribution across countries
with(data, prop.table(table(complain.connect, country), margin = 2))


## Complaints about slow response
# Base probability of a slow response complaint: 5%
prob_slow_response <- rep(0.05, data.n)
# Email contacts: 6x higher likelihood
prob_slow_response[data$method == "email"] <- prob_slow_response[data$method == "email"] * 6
# Chat and web_form: 3x higher likelihood
prob_slow_response[data$method %in% c("chat", "web_form")] <- prob_slow_response[data$method %in% c("chat", "web_form")] * 3
# Ensure probability values remain within [0, 1]
prob_slow_response [prob_slow_response > 1] <- 1
prob_slow_response [prob_slow_response < 0] <- 0
# Generate binary variable
data$complain.sp.respond <- rbinom(n = data.n, size = 1, prob = prob_slow_response)
# Check distribution across contact methods
with(data, prop.table(table(complain.sp.respond, method), margin = 2))


## Complaints about slow resolution
# Base probability of a slow resolution complaint: 10%
prob_slow_resolution <- rep(0.10, data.n)
# If contact is via chat and reason is incorrect_item → 5x higher likelihood
is_chat_incorrect <- data$method == "chat" & data$reason == "incorrect_item"
prob_slow_resolution[is_chat_incorrect] <- prob_slow_resolution[is_chat_incorrect] * 5
# Ensure probability values remain within [0, 1]
prob_slow_resolution [prob_slow_resolution > 1] <- 1
prob_slow_resolution [prob_slow_resolution < 0] <- 0
# Generate binary variable
data$complain.sp.solve <- rbinom(n = data.n, size = 1, prob = prob_slow_resolution)
# Check distribution by reason and method
with(data, prop.table(table(complain.sp.solve, reason, method), margin = c(2, 3)))


## Complaints about having to repeat information
# Base probability of a repetition complaint: 2%
prob_repeat_info <- rep(0.02, data.n)
# If contact reason is cancellation → 10x higher likelihood
prob_repeat_info[data$reason == "cancellation"] <- prob_repeat_info[data$reason == "cancellation"] * 10
# Ensure probability values remain within [0, 1]
prob_repeat_info [prob_repeat_info > 1] <- 1
prob_repeat_info [prob_repeat_info < 0] <- 0
# Generate binary variable
data$complain.repeat <- rbinom(n = data.n, size = 1, prob = prob_repeat_info)
# Check distribution across contact reasons
with(data, prop.table(table(complain.repeat, reason), margin = 2))


## Other types of complaints (uniform distribution)
# Constant probability of 20% for other unspecified complaints
data$complain.other <- rbinom(n = data.n, size = 1, prob = 0.20)
# Check distribution
with(data, prop.table(table(complain.other)))

```

## 3.3. Simulation of NPS scores 

We simulate NPS scores by first assuming a normally distributed base score in the ideal case where customers have no complaints.  

Then, we apply fixed score penalties based on the presence of different types of complaints.  
Finally, we constrain scores to the valid NPS range (0–10), convert them to integers, and add placeholders for open-text responses.

```{r}
## Simulation of NPS scores

# Step 1: Simulate base NPS scores assuming no complaints (mean = 9, sd = 2)
data$nps.score <- rnorm(data.n, mean = 9, sd = 2)

# Step 2: Apply fixed penalties based on complaints
# -3 for connection issues
# -4 for slow response
# -6 for slow resolution
# -2 for repeated information
# -1 for other complaints
data$nps.score <- data$nps.score -
  3 * data$complain.connect -
  4 * data$complain.sp.respond -
  6 * data$complain.sp.solve -
  2 * data$complain.repeat -
  1 * data$complain.other 

# Step 3: Ensure scores stay within the 0–10 range
data$nps.score[data$nps.score < 0] <- 0
data$nps.score[data$nps.score > 10] <- 10 

# Step 4: Convert to integer values (using floor)
data$nps.score <- floor(data$nps.score)

# Step 5: Add placeholder for open-text comments
data$open.comment <- rep("bla bla", data.n)

# Step 6: Visualize score distribution
barplot(table(data$nps.score))
with(data, prop.table(table(data$nps.score)))

```


### 3.3.1. Specifying the NPS segments based on NPS scores

Based on standard NPS segmentation, we classify customers into three categories:
- **Detractors**: scores from 0 to 6
- **Passives**: scores of 7 or 8
- **Promoters**: scores of 9 or 10

We create a new categorical variable to reflect these groupings.

```{r}
## Create NPS segments based on score values

# Step 1: Initialize empty character variable
data$nps.segment <- rep(NA_character_, data.n)

# Step 2: Assign segment based on score range
data$nps.segment[data$nps.score <= 6] <- "detractor"
data$nps.segment[data$nps.score > 6 & data$nps.score < 9] <- "passive"
data$nps.segment[data$nps.score >= 9] <- "promoter"

# Step 3: Convert to factor for categorical analysis
data$nps.segment <- factor(data$nps.segment, levels = c("detractor", "passive", "promoter"))

# Step 4: Check distribution across NPS segments
with(data, prop.table(table(nps.segment)))

```


## 3.4. Simulation of satisfaction scores

In addition to our main dependent variable (NPS), we simulate satisfaction scores, which were available for some customer service categories in the original project.

These scores will be useful to explore the **convergent validity** of NPS, as they are expected to be positively correlated.

```{r}
## Simulation of satisfaction scores (1 to 5 scale)

# Step 1: Generate satisfaction scores based on NPS
# We use a normal distribution centered at 3, with some variability (sd = 0.5)
# and add a scaled component based on NPS to simulate a positive relationship.
data$satisfaction <- floor(
  rnorm(data.n, mean = 3, sd = 0.5) + 
  0.9 * (1 + as.numeric(scale(data$nps.score)))
)

# Step 2: Ensure scores stay within the 1–5 range
data$satisfaction[data$satisfaction < 1] <- 1
data$satisfaction[data$satisfaction > 5] <- 5

# Step 3: Visualize the distribution
barplot(table(data$satisfaction))
with(data, prop.table(table(satisfaction)))

# Step 4: Check correlation with NPS scores (Spearman method)
with(data, cor(nps.score, satisfaction, method = "spearman"))


```


### 3.4.1. Simulating missing data for satisfaction

To reflect the restrictions observed in the original project, we simulate that satisfaction data is **not available** in the following conditions:

- For customers from **Drinkland**
- For contacts via **email** or **web form**

In addition, we assume that even when satisfaction was supposed to be collected, there is a **10% probability of missingness**, simulating cases where customers simply skipped the question.

This missing data was a key consideration in the original project and contributed to the decision to focus on likelihood to recommend (Net Promoter Score) rather than satisfaction, as the latter was not consistently available across segments.

```{r}
## Simulate missing data for satisfaction

# Step 1: Set satisfaction to missing for Drinkland
data$satisfaction[data$country == "Drinkland"] <- NA_real_

# Step 2: Set satisfaction to missing for email and web_form contacts
data$satisfaction[data$method %in% c("email", "web_form")] <- NA_real_

# Step 3: Introduce 10% additional random missingness among remaining valid values
# Create a random vector (0 = missing, 1 = keep) for non-NA entries
na_randomizer <- sample(
  c(0, 1),
  size = sum(!is.na(data$satisfaction)),
  replace = TRUE,
  prob = c(0.1, 0.9)
)
# Apply missing values where randomizer is 0
data$satisfaction[which(!is.na(data$satisfaction))[na_randomizer == 0]] <- NA_real_

# Step 4: Check final distribution of satisfaction (including NAs)
with(data, prop.table(table(is.na(satisfaction), method, country), margin = 2:3))


```

#4. Exploratory Analysis to Identify Key Areas for Improvement

#4.1. Measurement Considerations: Convergent Validity of NPS Scores

The validity of our main dependent variable, the Net Promoter Score (NPS), has been subject to criticism. Some of these critiques highlight that NPS relies on a single item, which may not fully capture the complexity of the likelihood to recommend. Additionally, the standard NPS formulation specifically asks about recommending the brand to friends or colleagues, which may not generalize to all recommendation scenarios.

To build confidence in the use of NPS within our context, we examine its convergent validity with satisfaction scores, which were available for a subset of markets and contact methods. A strong positive association between NPS and satisfaction would support the concurrent validity of NPS as a proxy for overall customer evaluation.


## 4.1.1. Preliminary considerations for correlations

To determine the most appropriate method for computing correlations, we began by exploring the distributions of both satisfaction and NPS scores. For this, we used bar charts generated with the ggplot2 package:


```{r}
# Distribution of NPS scores
ggplot(data[!is.na(data$nps.score), ], aes(x = factor(nps.score))) +
  geom_bar(fill = "#2980b9") +
  labs(
    title = "Distribution of NPS Scores",
    x = "NPS Score",
    y = "Count"
  ) +
  theme_minimal()

# Distribution of satisfaction scores
ggplot(data[!is.na(data$satisfaction), ], aes(x = factor(satisfaction))) +
  geom_bar(fill = "#27ae60") +
  labs(
    title = "Distribution of Satisfaction Scores",
    x = "Satisfaction Score",
    y = "Count"
  ) +
  theme_minimal()
```

Neither nps.score nor satisfaction follows a normal distribution. Both variables exhibit left-skewed patterns, with nps.score also showing signs of a ceiling effect, as a substantial proportion of scores accumulate near the top end of the scale.

Given these distributional characteristics, Pearson's correlation is not suitable—it assumes linearity, homoscedasticity, and approximately normal distributions, and is highly sensitive to skewed or bounded data.

To inform the selection of a more appropriate correlation method—such as Spearman’s rank correlation or polychoric correlation—we first inspect the relationship between nps.score and satisfaction visually. A scatterplot allows us to evaluate whether the association appears monotonic and to better understand its form and strength.

We use jittering to reduce overlap in the plot, given that both variables are discrete and bounded, which can otherwise obscure the pattern of relationships.

```{r}
# Create a scatterplot to visualize the relationship between NPS and satisfaction

ggplot(na.omit(data[, c("nps.score", "satisfaction")]), 
       aes(x = jitter(nps.score), y = jitter(satisfaction))) +
  geom_point(alpha = 0.5, color = "#2c3e50") +  # Add semi-transparent points for visual clarity
  labs(
    title = "Relationship between NPS Scores and Satisfaction",
    x = "NPS Score",
    y = "Satisfaction"
  ) +
  theme_minimal()  

```


The scatterplot reveals a strong, monotonic—and approximately linear—relationship between NPS score and satisfaction.


## 4.1.2. Spearman Correlation between NPS and Satisfaction Scores

Given that both variables are ordinal in nature and not normally distributed, we use Spearman’s rank correlation. This method evaluates the strength and direction of a monotonic association, without assuming interval-level measurement or normality.

```{r}
# Compute Spearman's rank correlation between NPS and satisfaction using the 'psych' package
with(data, corr.test(nps.score, satisfaction, method = "spearman", use = "complete.obs"))

```

The results indicate a strong positive Spearman correlation between satisfaction and NPS scores.
This supports the convergent validity of NPS in our dataset and suggests it is a meaningful proxy for overall customer satisfaction.


## 4.1.3. Further Measurement Considerations

Despite the evidence supporting the convergent validity of the **raw NPS scores**, there remain concerns about the use of the **categorical NPS metric**, which is calculated by subtracting the percentage of detractors (scores 0–6) from the percentage of promoters (scores 9–10).

This transformation introduces ambiguity for two reasons:  
1. **Loss of information**: Grouping continuous scores into categories discards nuance.  
2. **Distributional blindness**: The categorical NPS does not account for the full shape of the score distribution. For example, an NPS of 20 could result from 20% promoters and 0% detractors—or from 60% promoters and 40% detractors—two very different satisfaction profiles.

While we will use the categorical NPS in certain summaries and visualizations—given its widespread familiarity among stakeholders, we will systematically compare those results with analyses based on **raw NPS scores**, to ensure consistency and transparency.


## 4.2. Preliminary Inspection of Key Customer Service Factors with Heatmaps

Given the large number of levels across the variables contact method, contact reason, and country, it is important to **explore potential interactions** between these factors before proceeding to formal modeling.

It is crucial to consider these interactions in the modeling to avoid confusions, such as attributing an effect to one factor when it is actually driven by an interaction with another.

To visually inspect these relationships, we will use heatmaps to examine the frequency of different combinations of these categorical variables.


### 4.2.1. Reusable Function to Generate Heatmaps

We first define a function that generates summary heatmaps for any dependent variable and any aggregation function we wish to apply (e.g., mean, categorical NPS, etc.), across our selected factors. 

This function systematically provides results for all main effects as well as all levels of interaction between the factors, facilitating a comprehensive visual exploration of their relationships.

```{r}

f.heatmap.3f <- function(my.data, my.dv, factor.columns, factor.rows, factor.blocks, my.function) {

  # Initialize list to store results and a counter
  results <- list()
  k <- 1

  # Extract unique levels of each factor
  factor.column.levels <- unique(my.data[[factor.columns]])
  factor.rows.levels <- unique(my.data[[factor.rows]])
  factor.blocks.levels <- unique(my.data[[factor.blocks]])

  # 1. Overall results (no breakdown by block factor)

  # 1.1 First row: totals for whole dataset and totals by column factor
  row.label.t <- "Total all sample"

  # Calculate overall summary metric (excluding NAs)
  vector.total <- my.data[[my.dv]]
  vector.total.clean <- vector.total[!is.na(vector.total)]
  if (length(vector.total.clean) > 0) {
    total <- my.function(vector.total.clean)
  } else {
    stop("The dataset is empty")
  }

  # Calculate summary metric for each level of the column factor
  c.total <- sapply(factor.column.levels, function(c) {
    vector.c.total <- my.data[my.data[[factor.columns]] == c, my.dv]
    vector.c.total.clean <- vector.c.total[!is.na(vector.c.total)]
    if (length(vector.c.total.clean) > 0) {
      my.function(vector.c.total.clean)
    } else {
      NA_real_
    }
  })

  # Save totals row in results list
  results[[k]] <- c(group = row.label.t, overall = total, setNames(object = c.total, factor.column.levels))
  k <- k + 1

  # 1.2 Additional rows: results for each level of the row factor,
  # with column factor breakdowns

  for (r in factor.rows.levels) {
    row.label.r <- r

    r.total.vector <- my.data[my.data[[factor.rows]] == r, my.dv]
    r.total.vector.clean <- r.total.vector[!is.na(r.total.vector)]
    r.total <- if (length(r.total.vector.clean) > 0) my.function(r.total.vector.clean) else NA_real_

    rc.crossed <- sapply(factor.column.levels, function(c) {
      rc.crossed.vector <- my.data[
        my.data[[factor.rows]] == r & my.data[[factor.columns]] == c,
        my.dv
      ]
      rc.crossed.vector.clean <- rc.crossed.vector[!is.na(rc.crossed.vector)]
      if (length(rc.crossed.vector.clean) > 0) {
        my.function(rc.crossed.vector.clean)
      } else {
        NA_real_
      }
    })

    results[[k]] <- c(group = row.label.r, overall = r.total, setNames(object = rc.crossed, factor.column.levels))
    k <- k + 1
  }

  # 2. Results broken down by block factor

  for (b in factor.blocks.levels) {
    row.label.b <- paste0("Total: ", b)

    b.vector <- my.data[my.data[[factor.blocks]] == b, my.dv]
    b.vector.clean <- b.vector[!is.na(b.vector)]
    b.total <- if (length(b.vector.clean) > 0) my.function(b.vector.clean) else NA_real_

    bc.crossed <- sapply(factor.column.levels, function(c) {
      bc.crossed.vector <- my.data[
        my.data[[factor.blocks]] == b & my.data[[factor.columns]] == c,
        my.dv
      ]
      bc.crossed.vector.clean <- bc.crossed.vector[!is.na(bc.crossed.vector)]
      if (length(bc.crossed.vector.clean) > 0) my.function(bc.crossed.vector.clean) else NA_real_
    })

    results[[k]] <- c(group = row.label.b, overall = b.total, setNames(object = bc.crossed, factor.column.levels))
    k <- k + 1

    # 2.2 Rows within blocks: results for each row factor level within block,
    # with column factor breakdowns

    for (r in factor.rows.levels) {
      row.label.br <- paste0(b, ": ", r)

      br.crossed.vector <- my.data[
        my.data[[factor.blocks]] == b & my.data[[factor.rows]] == r,
        my.dv
      ]
      br.crossed.vector.clean <- br.crossed.vector[!is.na(br.crossed.vector)]
      br.crossed <- if (length(br.crossed.vector.clean) > 0) my.function(br.crossed.vector.clean) else NA_real_

      all.crossed <- sapply(factor.column.levels, function(c) {
        all.crossed.vector <- my.data[
          my.data[[factor.blocks]] == b &
          my.data[[factor.rows]] == r &
          my.data[[factor.columns]] == c,
          my.dv
        ]
        all.crossed.vector.clean <- all.crossed.vector[!is.na(all.crossed.vector)]
        if (length(all.crossed.vector.clean) > 0) my.function(all.crossed.vector.clean) else NA_real_
      })

      results[[k]] <- c(group = row.label.br, overall = br.crossed, setNames(object = all.crossed, factor.column.levels))
      k <- k + 1
    }
  }

  # 3. Combine results into a dataframe and convert numeric columns
  results.df <- as.data.frame(do.call(rbind, results), stringsAsFactors = FALSE)
  results.df[, -1] <- lapply(results.df[, -1], as.numeric)

  return(results.df)
}

```


### 4.2.2.Heatmap for NPS in Categorical Scores

We first created a heatmap using NPS in its categorical form, as this is the format most familiar to stakeholders. This choice facilitates collaboration and interpretation, enabling domain experts to visually identify potential patterns and effects across key service factors.

To do so, we implemented a simple, reusable function that computes the categorical NPS score from any vector of raw NPS values. This function includes checks for common data issues (e.g., missing or out-of-range values) and provides warnings when the sample size may be insufficient for stable estimates. 

```{r}
f.nps.cat <- function (my.nps.vector, na.rm=TRUE){ 
  # my.nps.vector is a numeric vector of raw NPS scores. It can contain NAs
  
  vector.c <- my.nps.vector[!is.na(my.nps.vector)] # vector cleaned of NAs
  
  # Error messages if sample size = 0 or if there are out-of-range NPS values:
  if(length(vector.c) == 0) {stop("error: no available NPS raw scores")} 
  if(length(vector.c[vector.c > 10]) > 0) {stop("error: scores higher than 10 in the NPS raw scores")} 
  if(length(vector.c[vector.c < 0]) > 0) {stop("error: scores lower than 0 in the NPS raw scores")} 
  
  # Warning if sample size is below 70 (low reliability)
  if(length(vector.c) < 70) {warning("few scores available for calculation (n<70)")} 
  
  n <- length(vector.c) # sample size (excluding NAs)
  prop.promoters <- (length(vector.c[vector.c >= 9])) / n # proportion of promoters
  prop.detractors <- (length(vector.c[vector.c <= 6])) / n # proportion of detractors
  nps.cat <- (prop.promoters - prop.detractors) * 100 # NPS score in categorical terms
  
  return(nps.cat) # returns NPS as a categorical score
}

# Check if it is working as expected
f.nps.cat(data$nps.score)
with(data, prop.table(table(nps.segment)))

```

The function is working correctly (we can confirm that the result matches the subtraction of the percentage of promoters and detractors).

We are now ready to generate the heatmap for the categorical NPS using the reusable functions defined earlier.

```{r}

# Use the function to calculate heatmaps
nps.cat.heatmap <- f.heatmap.3f(
  my.data = data,
  my.dv = "nps.score",
  factor.columns = "method",
  factor.rows = "reason",
  factor.blocks = "country",
  my.function = f.nps.cat  # Function to compute categorical NPS scores
)

# Export the results to Excel to apply conditional formatting (e.g., color scales)
write.xlsx(nps.cat.heatmap, "nps.cat.heatmap.xlsx", colnames = TRUE)

#Check if you have acceptable sample sizes for all cells, using also the heatmap function
nps.cat.heatmap.n <- f.heatmap.3f (
  my.data = data,
  my.dv = "nps.score",
  factor.columns = "method",
  factor.rows = "reason",
  factor.blocks = "country",
  my.function = length
)

```


All sample sizes across the heatmap cells are sufficiently large (n > 150), reducing the likelihood of sampling error and increasing confidence in the observed patterns.

After applying conditional formatting (color scales) in Excel, we obtain the heatmap, which helps us identify areas where the probability of recommending our customer service is particularly low. This visualization allows us to assess whether these effects are driven by main factors or by interactions between them.

```{r}

# Import the image of the heatmap generated in Excel with conditional formatting
knitr::include_graphics("Heatmap.nps.cat.png")

```

The heatmap reveals several **main effects**:

- Customers in Drinkland show a lower probability to promote compared to those in Eatland (see the yellow-colored area in the Drinkland block).

- Customers contacting for cancellations are less likely to promote than those contacting for other reasons (see the redder rows corresponding to cancellation reasons).

- Phone is associated with the highest probability to promote, while email shows the lowest (see the greener column for phone and the redder column for email).

More importantly, the heatmap helps us detect an **interaction** between contact reason and contact method:
Customers who contact through chat for incorrect item deliveries show a particularly low probability to promote (see the deep red cells at the intersection of the chat column and incorrect item row).

Before proceeding to statistically model these effects, we will check whether similar patterns emerge when using alternative outcome measures, such as raw NPS scores and satisfaction ratings.


### 4.2.3. Heatmap for NPS Raw Scores

To ensure that the limited categorization in the NPS categorical scores does not lead to misleading patterns or ambiguities, we examine whether similar patterns emerge when using the NPS raw scores. For this, we apply our reusable heatmap function to calculate the mean of the NPS raw scores across the factors.

```{r}

# Apply the heatmap function to NPS raw scores
nps.raw.heatmap <- f.heatmap.3f(
  my.data = data,
  my.dv = "nps.score",
  factor.columns = "method",
  factor.rows = "reason",
  factor.blocks = "country",
  my.function = mean  # Calculation of the mean of the NPS raw scores
)

# Export the heatmap data to Excel for conditional formatting (color scales)
write.xlsx(nps.raw.heatmap, "nps.raw.heatmap.xlsx", colnames = TRUE)

# Visualize the heatmap after adding colors in Excel
knitr::include_graphics("Heatmap.nps2.png")

```

We observe a similar pattern in both methods of calculating NPS results. However, it is also valuable to examine whether satisfaction scores reveal comparable patterns.


### 4.2.4. Heatmap for Satisfaction Scores

As further evidence of convergent validity between NPS and satisfaction scores, we assess whether these two measures show similar patterns across our factors.

To do this, we apply our reusable heatmap function to compute the mean satisfaction scores.

```{r}

# Apply the heatmap function to satisfaction scores
satisfaction.heatmap <- f.heatmap.3f(
  my.data = data,
  my.dv = "satisfaction",
  factor.columns = "method",
  factor.rows = "reason",
  factor.blocks = "country",
  my.function = mean  # Calculation of the mean satisfaction scores
)

# Export the heatmap data to Excel for conditional formatting (color scales)
write.xlsx(satisfaction.heatmap, "satisfaction.heatmap.xlsx", colnames = TRUE)

# Visualize all heatmaps after applying colors in Excel
knitr::include_graphics("Heatmap.png")

```

We observe a similar pattern for satisfaction and both methods of calculating NPS results, which increases our confidence in the validity of NPS scores within this context.


### 4.2.5. Correlations between Results of Different Dependent Variables

To further assess convergence between the dependent variables, we calculate Pearson correlations between their heatmap results. 

First, we reshape each heatmap dataframe into a long format with a single column for the values. Then, we merge these datasets and calculate the correlations.

```{r}
# Reshape heatmap results into long format
sat.long <- satisfaction.heatmap %>% pivot_longer(cols = -1,
                                 names_to = "evaluation",
                                 values_to = "satisfaction")

nps.raw.long <- nps.raw.heatmap %>% pivot_longer(cols = -1,
                                 names_to = "evaluation",
                                 values_to = "nps.raw")

nps.cat.long <- nps.cat.heatmap %>% pivot_longer(cols = -1,
                                 names_to = "evaluation",
                                 values_to = "nps.cat")

# Merge all heatmap results into a single dataset
merged.heatmaps <- merge(nps.cat.long, merge(sat.long, nps.raw.long, by = c("group", "evaluation")), by = c("group", "evaluation"))

# Calculate Pearson correlations across the three measures
cor(merged.heatmaps[, 3:5], use = "complete.obs")

```

All three measures show strong correlations (r > 0.90), providing additional confidence that the NPS is a valid measure and that important patterns are not missed by relying on any single metric.


## 2.3. Statistical Models about Factors Related to Probability to Promote. 

While the visualization of the heatmaps was already informing us about important aspects in which customer service is performing lower, we further explore these relations with statistical models.

### 2.3.1. Building an Ordinal logistic model. 

We consider that an ordinal logistic model is more appropriate than a lineal model because our variable to predict, nps scores, is an ordinal variable in which it is difficult to assume similar distances across all of the scores, specially due to the ceiling and floor effects that we infer from its distribution (see section x). 

Furthermore, a linear model would not fulfill the assumption for the normality of residuals, as we can see in the QQ plots below, even when we use a square transformation of the variable.  

```{r}
#Checking normality and homocedasticy of residuals of the linear model
linear.model <- lm (nps.score ~ method + reason + country, data =data)
par(mfrow =c(2,2))
plot(linear.model)

#Checking normality and homocedasticy of residuals of the linear model with the squared transformation
linear.model.sq <- lm ((nps.score^2) ~ method + reason + country, data =data)
par(mfrow =c(2,2))
plot(linear.model.sq)

```


As we can see in the Q-Q plots, the residuals' distributions differ strongly from the hypothetical normal distribution. 

At this point, we also considered that an ordinal model was preferable than a binomial logistic regression, because it would provide information about the effects of the factors across the whole NPS scale, rather than just on a single NPS category.  

It is important to note that we also had the option to use a 
For all of these reasons, we proceed with ordinal logistic regression. 


###2.3.1.1. Setting References across the Categories of the Factors

Just to make the interpretation more intuitive, we set the levels of our factors in a way that the levels with higher nps scores are specified first (following the heatmap we calculated earlier). That way, the best levels will be used as reference to compare the effect of the others. 


```{r}
data$method <- factor(data$method, levels = c("phone", "web_form", "chat", "email"))
data$reason <- factor(data$reason, levels = c("other","status", "delay", "cancellation", "incorrect_item"))
data$country <- factor(data$country, levels = c("Eatland", "Drinkland"))

```



### 2.3.1.2. Checking the collinearity assumption

We check the collinearity across our categorical factors using visual representations and VIF scores.

```{r}
# Mosaic plot to visualize the relations between factors (using the vcd package)
doubledecker(with(data, table(country, reason, method )))

# VIF coefficients, extracted from the linear model we calculated in section x (using the car package)
vif(linear.model)

```

The VIF analysis indicates an absence of multicollinearity among our predictors. For all factors, we used the the GVIF^(1/(2*Df)) values, the most appropriate metric for comparing predictors with different numbers of categories—are extremely close to 1.0.

This finding is further supported by the mosaic plot, which visually demonstrates that the frequencies of one factor are consistent across the levels of the other factors, showing no strong interdependencies



##### 2.3.1.3. Checking the Proportional odds assumption


Before testing this assumption we create dummy variables for the factors with multiple categories, so we can see the results of the proportional odds separately for each of the variables. 

We also make sure our dependent variable is specified as ordinal

```{r}

# Dummy variables for method, using phone as the reference
data$chat <- NA_real_
data$chat[data$method == "chat"] <- 1
data$chat[!(data$method == "chat")] <- 0

data$email <- NA_real_
data$email[data$method == "email"] <- 1
data$email[!(data$method == "email")] <- 0

data$web_form <- NA_real_
data$web_form[data$method == "web_form"] <- 1
data$web_form[!(data$method == "web_form")] <- 0

# Dummy variables for reason, using status as the reference
data$delay <- NA_real_
data$delay[data$reason == "delay"] <- 1
data$delay[!(data$reason == "delay")] <- 0

data$cancellation <- NA_real_
data$cancellation[data$reason == "cancellation"] <- 1
data$cancellation[!(data$reason == "cancellation")] <- 0

data$incorrect_item<- NA_real_
data$incorrect_item[data$reason == "incorrect_item"] <- 1
data$incorrect_item[!(data$reason == "incorrect_item")] <- 0

data$status <- NA_real_
data$status[data$reason == "status"] <- 1
data$status[!(data$reason == "status")] <- 0


# Register the nps score as an ordinal factor
data$nps.score <- factor( data$nps.score, levels = 0:10, ordered = TRUE)

```



We check the proportional odds assumption with the ordinal package

```{r}


#Specify the basic ordinal model with the main effects
ordinal.model <- clm(nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item  + country, data = data)
summary (ordinal.model)
#Check the proportional odds assumption
nominal_test(ordinal.model)



```

The results indicate that, with the exception of web_form, all the factors for contact methods and contact reasons violate the proportional odds assumption, that is, that their effects on a customer's NPS score is not consistent across the entire 0-10 scale.


Attending to this, we will check if this issue can be corrected by incorporating interactions between the method and reason factors that we have identified in our heatmap. We can also see if simplifying the model can help

If not, we will try a more flexible model that calculate separate set of coefficients for method and reason at each threshold of the nps scores.


##### 2.3.1.3 Addressing Non-Proportional Odds by Incorportating Interactions into the Model

In the heatmaps we observed an important interaction: customers contacting through chat for incorrect item deliveries feel very low probability to promote our customer service. 

The incorporation of this interaction to the model not only can be informative, it could eliminate the non-proportional odds that we had identified for the method and the reason factors. 

We create this model using the ordinal package: 

```{r}

# Prior step: creation of a dicotomous variable to account for the interaction: whether cases are due to incorrect item cases and attended by chat

data$chat_incorrect_item <- NA_real_
data$chat_incorrect_item[data$method == "chat" & data$reason == "incorrect_item"] <- 1
data$chat_incorrect_item[!(data$method == "chat" & data$reason == "incorrect_item")] <- 0

with(data,(table(chat_incorrect_item, method, reason))) # to check the variable has been created as expected

# We build the ordinal logistic model with the ordinal package: 
ordinal.model.int <-  clm(nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item  + country + chat_incorrect_item,  data = data) 

summary(ordinal.model.int)

anova(ordinal.model, ordinal.model.int)


```



```{r}

```

The model shows that the interaction is significant. Furthermore, the likelihood ratio test indicate a statistically significant improvement in model fit for the more complex model that includes the chat_incorrect_item interaction. This is supported by a significant chi-squared test (p < 2.2e-16) and a significantly lower AIC (161,578.4) compared to the simpler model with only main effects (AIC = 161,830.7). 


Now, let's check if the proportional odds assumption of the new model is fulfilled


```{r}

#Check the proportional odds assumption
nominal_test(ordinal.model.int)
```

The inclusion of the interaction has improved the fulfillment of the proportional odds for some factors. It is now fulfilled for the contact reason of delay. Since this factor is not associated with a significant difference in comparison with the other contact reasons used as a reference, we could take it away from the model. 

However, non proportional odds are still problematic for other contact reasons and contact methods. To further explore how these non proportional odds are acting we visualize jitter plots with the ggplot2 package: 

```{r}

#Reusable function for combining jitter plots and box plots: 

f.nps.jitter_box <- function (my.data, my.factor, my.dv){
  ggplot(my.data, aes(x = factor(.data[[my.factor]]), y = as.numeric(as.character(.data[[my.dv]])))) +
  geom_jitter(width = 0.4, alpha = 0.1, color = "blue") + # Adjusted width and alpha for clarity
  geom_boxplot(alpha = 0, outlier.shape = NA) + # Add transparent boxplots to show median/quartiles
  labs(title = paste("NPS Score by", my.factor),
       y = my.dv,
       x = my.factor) +
  theme_minimal() +
  scale_y_continuous(breaks = 0:10) + # Ensure all NPS scores are ticks
  scale_x_discrete(labels = c("No", "Yes"))
}

#Application of the function to each factor

# For people contacting through chat:      
f.nps.jitter_box(data, "chat", "nps.score")

# For people contacting through email
f.nps.jitter_box(data, "email", "nps.score")

# For people contacting for status issues
f.nps.jitter_box(data, "status", "nps.score")

# For people contacting for cancellation issues
f.nps.jitter_box(data, "cancellation", "nps.score")

# For people contacting for incorrect item issues
f.nps.jitter_box(data, "incorrect_item", "nps.score")

# For people contacting for incorrect item issues via chat
f.nps.jitter_box(data, "chat_incorrect_item", "nps.score")
```
The graphs illustrate that the influence of the factors that do not fulfill the proportional odds assumption appears to be stronger in the lower intervals of the NPS scale (i.e., differences more marked in the first quartile and below it). That is, the factors appear to have effect in the creation of detractors.  

To better address these non-proportional odds, we will try to account for them in our model. 

##### 2.3.1.4. Addressing Non-Proportional Odds by Accounting for Them in the Models

We build a model that assumes that the influence of factors with non-proportional odds can have different coefficients for each interval of the NPS scores, using the VGAM package

We also take away from the model the factor of contacting for delays, since this factor fulfilled proportional odds and was not associated with significant differences in the nps scores. 

```{r}


# Model where SOME predictors are assumed to be not proportional
ordinal.non_proportional.model.int <- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item  + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form),
  data = data
)

summary(ordinal.non_proportional.model.int) 
```
The model did not fit correctly, and it is showing warnings that it is over-predicting some outcomes, leading to unstable calculations.

We will try a simple model in which we only assume as non-proportional in the ones in which this lack of odds proportionality is higher: the contact reason of incorrect items, and the interaction factor of incorrect items attended by chat: 



```{r}
# Model where fewer predictors are assumed to be not proportional
ordinal.non_proportional.model.int2 <- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item  + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form + chat + email + status + cancellation),
  data = data
)

summary(ordinal.non_proportional.model.int2)


```

This model still did not fit correctly, and it is showing warnings that it is leading to unstable calculations. We will try an even more simple model in which we only assume as non-proportional the interaction factor of incorrect items attended by chat: 


```{r}
# Model where only the interaction is assumed to be not proportional
ordinal.non_proportional.model.int3 <- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item  + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form + chat + email + status + cancellation + incorrect_item),
  data = data
)

summary(ordinal.non_proportional.model.int3)
```

This model still did not fit correctly, indicating that our data does not work well with non proportional assumptions. 


### 2.3.1.5. Unsolvable limitations of the ordinal model due non-proportional odds

Attending to the difficulties to assume non-proportional odds in our model, we considered that the prior ordinal model is the best model we can estimate with ordinal regression, whose summary we recall below: 

```{r}
summary (ordinal.model.int)
```


Yet, we should interpret this model with caution, considering that the effects represented by the coefficients are variable depending on the intervals of the nps scores. Specifically, they appear to be more intense in the lower intervals of the NPS. Attending to this, we provide a complementary model in which we estimate the probability of obtaining scores in these lowers intervals, specifically the scores from 0 to 6, which are generally considered as detractors. 


### 2.3.2. Binomial Logistic Regression Model to Predict Detractors

A binomial model to predict detractors not only is a way to bypasss the proportional odds assumption of the ordinal model,  we also expect to show the most important effects in our data, since we have observed that the effects of the factors was especially reflected in the scores of the nps below 5 (see graphs). 


```{r}

#Create a dicotomous variable for users who are detractors

data$detractor <- NA

data$detractor [data$nps.segment == "detractor"] <- 1
data$detractor [!(data$nps.segment == "detractor")] <- 0


# Fit the binomial model
binomial.model <- glm(detractor ~ web_form + chat + email + status + cancellation + incorrect_item  + country + chat_incorrect_item, data = data, family = "binomial")

summary(binomial.model)

```

The results are coherent with the ordinal model, but more stable and especifically focused on the presence of detractors. 

The contact reason status is not showing any significant effect, so we test if we can simplify the model by removing it: 

```{r}
# Fit the binomial model cleaned for irrelevant factors
binomial.model.c <- glm(detractor ~ web_form + chat + email + cancellation + incorrect_item  + country + chat_incorrect_item, data = data, family = "binomial")

summary(binomial.model.c)

anova(binomial.model.c, binomial.model, test ="Chisq")


```
Based on our analysis, the status variable does not have a significant impact on predicting whether a customer will be a detractor.We will therefore remove it to simplify the model, allowing us to focus on the most relevant and actionable drivers.







### 2.3.3. Creating Metrics to Interpret the Model

This section uses the information from the logistic regression model above to provide metrics of how much each factor is associated with customers becoming detractor, including metrics like the odds ratios, the relative proportion, the predicted proportion, and the differences in the predicted proportions. 

For the predicted proportions, we need to create a matrix about cases that represent the individual presence of the factors and the presence of the intercept. We start specifying this matrix: 

```{r}

#####
#Design Matrix for Predictions
#####

# Extract the factor names from the model, with and without the intercept
f.names.interc <- names(coef(my.model))
f.names <- f.names.interc [-1]


# Create a design matrix representing the individual presence of the factors and the intercept 
my.matrix.df <- as.data.frame(diag(length(f.names.interc)))
my.matrix.df [,1] <- f.names.interc
names(my.matrix.df) <- c("case", f.names)


# Specify the values for the interactions
my.matrix.df$chat[my.matrix.df$case == "chat_incorrect_item"] <- 1
my.matrix.df$incorrect_item[my.matrix.df$case == "chat_incorrect_item"] <- 1


# Make sure the variables are expressed like in the original dataset. We correct it for country: 
names(my.matrix.df)[names(my.matrix.df) == "countryDrinkland"] <- "country"
my.matrix.df$country[my.matrix.df$country==1] <-"Drinkland"
my.matrix.df$country[my.matrix.df$country==0] <-"Eatland"
my.matrix.df$country<-as.factor(my.matrix.df$country)

#Make sure the results are as expected
str(my.matrix.df)

```

Next, we create a function to obtain the metrics we are interested in from the binomial model and the specified matrix. Specifically, we will obtain odds ratios, relative proportions, predicted proportions, and absolute differences for the predicted proportions 

The formula also provides the 95% confidence intervals for these metrics. Please note that, for calculating the confidence intervals for the differences in the predicted proportion, we employ parametric bootstrap simulations. 


```{r}
#####
#Formula to obtain metrics to interpret a binomial logistic model
#####


f.metrics.binomial <- function(my.model, my.matrix){
  
  #--------1. Preliminary Checks: 
  
  # Making sure the model is glm binomial
  if (!inherits(my.model, "glm") || my.model$family$family != "binomial") {
    stop("The function requires a binomial model.")
  }
  
  
  #--------2. Calculation of Odds Ratios
  
  # Obtaining the coefficients and confidence intervals
  coefs <- coef(my.model)
  ci <- confint(my.model)
  intercept <- coefs[1]
  
  # Exponentiation of the coefficients and their CIs to obtain odds ratios
  or <- exp(coefs)
  or_ci <- exp(ci)
  
  
  #--------3. Calculating the Relative Probabilities (Risk Ratios)
  
  # Calculating the base probability from the intercept
  p0 <- or[1] / (1 + or[1])
  
  # Obtaining relative probabilities from risk ratios and base probability
  rp <- or / ((1 - p0) + (p0 * or))
  rp_ci_low <- or_ci[, 1] / ((1 - p0) + (p0 * or_ci[, 1]))
  rp_ci_high <- or_ci[, 2] / ((1 - p0) + (p0 * or_ci[, 2]))
  
  
  #--------4. Calculate the Predicted Proportions
  
  # Extract log-odds and their sd for the individual presence of the factors (using the specified matrix)
  pred <- predict(my.model, newdata = my.matrix, type = "link", se.fit = TRUE)

  #Calculate confidence intervals in log-odds
  lower_logit <- pred$fit - 1.96 * pred$se.fit
  upper_logit <- pred$fit + 1.96 * pred$se.fit

  #Specify a function to transform log-odds to probabilities
  f.inv_logit <- function(x) {1 / (1 + exp(-x))}
  
  #Apply the function to the log-odds and the limits of their ci
  predicted_prob <- f.inv_logit(pred$fit) # Predicted probabilities
  lower_predicted_prob <- f.inv_logit(lower_logit) # Lower limit of CI for predicted probabilities
  upper_predicted_prob <- f.inv_logit(upper_logit) # Upper limit of CI for predicted probabilities

  
  #--------5. Differences of the Predicted Proportions in Comparison to the Intercept

  # Simulate a distribution of log-odds, for example with 10000 cases
  set.seed(123)
  simulated_log_odds <- as.data.frame(matrix(
  rnorm(10000 * length(pred$fit), mean = pred$fit, sd = pred$se.fit),
  ncol = length(pred$fit), byrow = TRUE
  ))

  # Transform logg-odds to expected probabilities (using the formula specified above)
  simulated_probs <- f.inv_logit(simulated_log_odds)

  # Calculate differences from the first column, which correspond to the intercept probabilities
  diffs <- apply(simulated_probs, 2, function(col){
  col - simulated_probs[[1]]
  })

  # Calculate the means of the differences and 95% CI with percentiles
  prob_diff <- colMeans(diffs)
  lower_prob_diff  <- apply(diffs, 2, quantile, probs = 0.025)
  upper_prob_diff <- apply(diffs, 2, quantile, probs = 0.975)
  
  
  #-------- 6. Combining the results in a dataframe
  
  #Combining all metrics, and expressing predictions and predicted differences in percentages
  results <- data.frame(
    Term = names(rp),
    Prediction = as.numeric(predicted_prob)*100,
    Lower_Prediction = as.numeric(lower_predicted_prob)*100,
    Upper_Prediction = as.numeric(upper_predicted_prob)*100,
    Predicted_Change = as.numeric(prob_diff)*100,
    Lower_Predicted_Change = as.numeric(lower_prob_diff)*100,
    Upper_Predicted_Change = as.numeric(upper_prob_diff)*100,
    Relative_Probability = as.numeric(rp),
    Lower_Relative_Probability = as.numeric(rp_ci_low),
    Upper_Relative_Probability = as.numeric(rp_ci_high),
    Odds_Ratio = as.numeric(or),
    Lower_Odds_Ratio = as.numeric(or_ci[,1]),
    Upper_Odds_Ratio = as.numeric(or_ci[,2])
)
  
  # Deleting values assigned to the intercept that have no interpretation
  results[1, c("Predicted_Change",
             "Lower_Predicted_Change",
             "Upper_Predicted_Change",
             "Relative_Probability",
             "Lower_Relative_Probability",
             "Upper_Relative_Probability",
             "Odds_Ratio",
             "Lower_Odds_Ratio",
             "Upper_Odds_Ratio")] <- NA_real_

  
  # Return the results
  return(results)
}


```



Now we are ready to apply the function to our model to obtain our metrics: 

```{r}
#Application of the function to our binomial model
model.metrics.df <- f.metrics.binomial (binomial.model.c, my.matrix.df)
model.metrics.df

```


For making the interpretations easier, we rearrange the order of the results by the predicted differences: 

```{r}
#Arrange results from greater percentage change to lower, leaving the intercept above the dataframe:
int.df <- model.metrics.df[1,]
factors.df <- arrange(model.metrics.df[-1,], desc(Predicted_Change))
ord.model.metrics.df <- bind_rows(int.df, factors.df)
ord.model.metrics.df

```


We can also facilitate interpretation by creating labels that are easy to understand:

```{r}

# We provide labels for each factor in the results dataframe: 

ord.model.metrics.df$Label <- NA_character_
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "(Intercept)"] <- "Baseline: Typical issues attended via phone in Eatland"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "chat_incorrect_item"] <- "Attention via chat of incorrect item issues"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "email"] <- "Attention via email"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "chat"] <- "Attention via chat"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "web_form"] <- "Attention via web form"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "countryDrinkland"] <- "Attention in the country Drinkland"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "cancellation"] <- "Attention of cancellation issues"
ord.model.metrics.df$Label [ord.model.metrics.df$Term == "incorrect_item"] <- "Attention of incorrect items issues"
ord.model.metrics.df # to check the results are as expected

```


Lastly, we can graphically represent the results to easily see which key factors in customer service are associated with more customers becoming detractors. We will especficically represent the predictions. 


```{r}

# Create the dot-and-whisker plot for the predictions
ggplot(ord.model.metrics.df, aes(x = Prediction, 
                    y = reorder(Label, Prediction))) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgrey") +
  geom_errorbarh(aes(xmin = Lower_Prediction, xmax = Upper_Prediction), height = 0.2) +
  geom_point(size = 3) +
    geom_text(aes(label = paste(round(Prediction,1), "%")), 
            vjust = -1, size = 3.5) + 
  labs(
    title = "Expected Detractors By Key Customer Service Factors",
    x = "% of Detractors",
    y = "Customer Service Factors"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```

In the graph we can see that contacting with customer service through methods other than telephone is associated with more detractors, especially when contacting via email, or via chat in the specific case of solving incorrect item issues. 

Customer service in the country Drinkland is also associated with more detractors than in Eatland. 

Lastly, customers who contact for cancellation issues are expected to have more detractors than typical issues. 

However, it is important to consider that the impact of these situations might depend on how many customers contact us for each of them. We consider this in the next section. 


### 2.3.4. Evaluating the Impact of Customer Service Factors

To evaluate the impact in terms of additional detractors associated to each customer service factors, we will ponderate the predicted proportions in detractors by the number of cases associated to each factor. 

We assume that we have the following percentage of cases for each of the above factors: 
- Baseline: attention of typical issues via phone in Eatland: 10%
- Attention via chat of incorrect item issues: 10%
- Attention via email: 20%
- Attention via chat: 40%
- Attention via web form: 5%
- Attention in the country Drinkland: 50%
- Attention of cancellation issues: 5%
- Attention of incorrect items issues: 30%


We create impact variables by ponderating the predicted percentages of detractors and the differences in these percentages across factors: 

```{r}


#Create a variable specifying the proportion of cases
ord.model.metrics.df$prop.cases <- c(.10,.10, .20, .40, .05, .50, .05, .30)

#Create the impact variables
variables_to_mutate <- c( "Prediction",
                          "Lower_Prediction",
                          "Upper_Prediction", 
                          "Predicted_Change",
                          "Lower_Predicted_Change",
                          "Upper_Predicted_Change"
)

for(variable in variables_to_mutate){
  new_col <- paste0("Impact_", variable)
  ord.model.metrics.df[[new_col]] <-  ord.model.metrics.df[["prop.cases"]] * ord.model.metrics.df[[variable]]
}

# Check the results
ord.model.metrics.df

```

We can graphically represent the results for the additional predictors associated to each customer service factor
 

```{r}

# Create the dot-and-whisker plot for the additional detractors associated to each case.
ggplot(ord.model.metrics.df [-1,], aes(x = Impact_Predicted_Change, 
                    y = reorder(Label, Impact_Predicted_Change))) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "darkgrey") +
  geom_errorbarh(aes(xmin = Impact_Lower_Predicted_Change, xmax = Impact_Upper_Predicted_Change), height = 0.2) +
  geom_point(size = 3) +
    geom_text(aes(label = paste(round(Impact_Predicted_Change, 1), "%")), 
            vjust = -1.5, size = 3.5) + 
  labs(
    title = "Impact of Key Customer Service Factors",
    x = "% of Additional Detractors Compared to Baseline",
    y = "Customer Service Factors"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = "bold"))

```













































### Selection of the sample for the analyses of complaints

Although we initially simulated complaints for all subjects in the dataset, we didn't have this data available for all our cases. This data came from further coding for subgroups we wanted to do follow-up analyses (see Section X). Based on this, we're now selecting a stratified sample of 100 cases for each of the available contact methods, for each of the four contact reasons of interest ("incorrect_item", "delay", "status", "cancellation"), and for each of the two relevant countries. This ensures a balanced representation across key dimensions for our specialized analyses.

Specifically, for each method, reason, and country combination, we aim to obtain a sample of 100 subjects and store the results in a new dataset.

```{r}
# Función para obtener las métricas de impacto de un modelo binomial

f.metrics.binomial <- function(my.model){
  
  if (!inherits(my.model, "glm") || my.model$family$family != "binomial") {
    stop("La función requiere un objeto de modelo glm con familia 'binomial'.")
  }
  
  # 1. Crear un dataframe con los escenarios a comparar
  #    Necesitas definir todas tus variables dummy y de interacción
  escenarios <- data.frame(
    Term = names(coef(my.model)),
    web_form = c(0, 1, 0, 0, 0, 0, 0),
    chat = c(0, 0, 1, 0, 0, 0, 0),
    email = c(0, 0, 0, 1, 0, 0, 0),
    cancellation = c(0, 0, 0, 0, 1, 0, 0),
    incorrect_item = c(0, 0, 0, 0, 0, 1, 0),
    countryDrinkland = c(0, 0, 0, 0, 0, 0, 1)
  )
  
  # Eliminar la fila del intercepto del dataframe
  escenarios <- escenarios[-1, ]
  
  # 2. Predecir las probabilidades para cada escenario
  #    'type = "response"' devuelve la probabilidad directamente
  probabilidades_predichas <- predict(my.model, newdata = escenarios, type = "response")
  
  # 3. Calcular la probabilidad base
  #    Esto se hace prediciendo para el escenario de referencia (todo en 0)
  prob_base <- predict(my.model, newdata = data.frame(web_form = 0, chat = 0, email = 0, cancellation = 0, incorrect_item = 0, countryDrinkland = 0), type = "response")
  
  # 4. Calcular la diferencia absoluta en puntos porcentuales
  diferencia_absoluta_pp <- (probabilidades_predichas - prob_base) * 100
  
  # 5. Crear el dataframe final
  results <- data.frame(
    Term = escenarios$Term,
    Probabilidad_Base_pct = prob_base * 100,
    Nueva_Probabilidad_pct = probabilidades_predichas * 100,
    Diferencia_Absoluta_pp = diferencia_absoluta_pp
  )
  
  # 6. Ordenar los resultados
  results <- dplyr::arrange(results, desc(Diferencia_Absoluta_pp))
  
  return(results)
}

diff.prop.df <- f.metrics.binomial(binomial.model.c)
diff.prop.df

```


correlación y scatterplot
código para la matriz de NPS
con algún análisis acompañando: ver si uso modelo lineal o logarítmico






