<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Predicting Customer Detractors (Part 1). Analyzing Contextual Factors Via Logistic Regression</title>

<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/header-attrs-2.29/header-attrs.js"></script>
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/navigation-1.1/tabsets.js"></script>
<link href="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/highlightjs-9.12.0/highlight.js"></script>
<link href="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/pagedtable-1.1/js/pagedtable.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div id="header">



<h1 class="title toc-ignore">Predicting Customer Detractors (Part 1).
Analyzing Contextual Factors Via Logistic Regression</h1>

</div>


<div id="introduction" class="section level1">
<h1>1. Introduction</h1>
<p>This case study explores opportunities to improve customer service,
focusing on increasing the likelihood that customers recommend the
company.</p>
<p>The project was conducted in two phases:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Exploratory analysis</strong> to identify areas needing
improvement, based on internal data analysis and logistic regression
modeling. This is the phase represented in this document.</p></li>
<li><p><strong>Identifying potential improvements and modeling their
impact</strong>, using text analysis and simulation to evaluate how
addressing pain points could enhance customer perceptions. Due to length
constraints, this second part is presented in a separate document:
“Predicting Customer Detractors (Part 2): Assessing Improvement
Opportunities via Text Analysis.”</p></li>
</ol>
<p>Although based on a real-world project, all data, variables, and
insights presented here have been simulated to maintain
confidentiality.</p>
<p>The analysis includes:</p>
<ul>
<li>Data simulation and cleaning</li>
<li>Visualization techniques (e.g., heatmaps for high-dimensional
data)</li>
<li>Descriptive statistics</li>
<li>Regression modeling evaluations (linear, ordinal logistic, and
binomial logistic)</li>
<li>Simulation-based recommendations</li>
<li>Creation of reusable functions to automate procedures</li>
</ul>
</div>
<div id="setup" class="section level1">
<h1>2. Setup</h1>
<p>We start by loading the required packages for data manipulation,
visualization, modeling, and exporting results.</p>
<pre class="r"><code># Data handling
library(readxl)      # Read Excel files
library(openxlsx)    # Write Excel files
library(dplyr)       # Data manipulation</code></pre>
<pre><code>## 
## Adjuntando el paquete: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(tidyr)       # Data reshaping (long/wide formats)

# Visualization
library(ggplot2)     # General plotting
library(coefplot)    # Visualize model coefficients</code></pre>
<pre><code>## Warning: package &#39;coefplot&#39; was built under R version 4.5.1</code></pre>
<pre class="r"><code>library(vcd)         # Visualizing categorical data</code></pre>
<pre><code>## Warning: package &#39;vcd&#39; was built under R version 4.5.1</code></pre>
<pre><code>## Cargando paquete requerido: grid</code></pre>
<pre class="r"><code># Statistical analysis
library(psych)       # Descriptive statistics</code></pre>
<pre><code>## 
## Adjuntando el paquete: &#39;psych&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ggplot2&#39;:
## 
##     %+%, alpha</code></pre>
<pre class="r"><code>library(ordinal)     # Ordinal logistic regression</code></pre>
<pre><code>## 
## Adjuntando el paquete: &#39;ordinal&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     slice</code></pre>
<pre class="r"><code>library(VGAM)        # Alternative ordinal modeling</code></pre>
<pre><code>## Warning: package &#39;VGAM&#39; was built under R version 4.5.1</code></pre>
<pre><code>## Cargando paquete requerido: stats4</code></pre>
<pre><code>## Cargando paquete requerido: splines</code></pre>
<pre><code>## 
## Adjuntando el paquete: &#39;VGAM&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:ordinal&#39;:
## 
##     dgumbel, dlgamma, pgumbel, plgamma, qgumbel, rgumbel, wine</code></pre>
<pre><code>## The following objects are masked from &#39;package:psych&#39;:
## 
##     fisherz, logistic, logit</code></pre>
<pre class="r"><code>library(car)         # VIF and regression diagnostics</code></pre>
<pre><code>## Cargando paquete requerido: carData</code></pre>
<pre><code>## 
## Adjuntando el paquete: &#39;car&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:VGAM&#39;:
## 
##     logit</code></pre>
<pre><code>## The following object is masked from &#39;package:psych&#39;:
## 
##     logit</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     recode</code></pre>
</div>
<div id="data-simulation" class="section level1">
<h1>3. Data Simulation</h1>
<div id="simulating-demographic-and-contextual-variables"
class="section level2">
<h2>3.1. Simulating Demographic and Contextual Variables</h2>
<p>To create a realistic dataset, we simulate basic
<strong>demographic</strong> information (e.g., age, gender) and
<strong>contextual variables</strong> related to the customer service
experience: country, contact method, and reason for contacting.</p>
<pre class="r"><code># Set seed for reproducibility
set.seed(1000)

# Define dataset size 
data.n &lt;- 40000  # Large enough to allow subgroup analysis despite unbalanced category probabilities

# Initialize dataframe with IDs
data &lt;- data.frame(id = factor(1:data.n))

# Simulate age with a log-normal distribution, capped between 15 and 90
data$age &lt;- round(rlnorm(n = data.n, meanlog = log(40), sdlog = log(1.4)))
data$age[data$age &gt; 90] &lt;- 90
data$age[data$age &lt; 15] &lt;- 15

# Simulate gender
data$gender &lt;- factor(sample(c(&quot;man&quot;, &quot;woman&quot;),
                             prob = c(0.5, 0.5),
                             replace = TRUE, size = data.n))

# Simulate reason for contacting
data$reason &lt;- factor(sample(c(&quot;incorrect_item&quot;, &quot;delay&quot;, &quot;status&quot;, &quot;cancellation&quot;, &quot;other&quot;),
                             prob = c(0.20, 0.30, 0.20, 0.10, 0.20),
                             replace = TRUE, size = data.n))

# Simulate contact method
data$method &lt;- factor(sample(c(&quot;chat&quot;, &quot;web_form&quot;, &quot;email&quot;, &quot;phone&quot;),
                             prob = c(0.40, 0.10, 0.15, 0.35),
                             replace = TRUE, size = data.n))

# Simulate country (two fictional countries for anonymity)
data$country &lt;- factor(sample(c(&quot;Drinkland&quot;, &quot;Eatland&quot;),
                              prob = c(0.5, 0.5),
                              replace = TRUE, size = data.n))</code></pre>
</div>
<div id="simulation-of-customer-complaints" class="section level2">
<h2>3.2. Simulation of Customer Complaints</h2>
<p>We simulate five types of customer complaints: connection issues,
slow response, slow resolution, repetition of information, and other
unspecified complaints. These are conditioned on demographic and
contextual variables as follows:</p>
<ul>
<li><strong>Connection issues</strong>: more likely in Drinkland</li>
<li><strong>Slow response</strong>: more frequent in email, and
moderately in chat/web_form</li>
<li><strong>Slow resolution</strong>: more frequent when
“incorrect_item” is handled via chat</li>
<li><strong>Repetition</strong>: more likely when the contact reason is
“cancellation”</li>
<li><strong>Other complaints</strong>: uniformly distributed</li>
</ul>
<pre class="r"><code>## Complaints about connection issues
# Base probability of a connection complaint: 10%
prob_connection &lt;- rep(0.10, data.n)
# In Drinkland, probability is doubled
prob_connection[data$country == &quot;Drinkland&quot;] &lt;- prob_connection[data$country == &quot;Drinkland&quot;] * 2
# Ensure probability values remain within [0, 1]
prob_connection [prob_connection &gt; 1] &lt;- 1
prob_connection [prob_connection &lt; 0] &lt;- 0
# Generate binary variable based on probabilities
data$complain.connect &lt;- rbinom(n = data.n, size = 1, prob = prob_connection)
# Check distribution across countries
with(data, prop.table(table(complain.connect, country), margin = 2))</code></pre>
<pre><code>##                 country
## complain.connect Drinkland   Eatland
##                0 0.7981113 0.8995473
##                1 0.2018887 0.1004527</code></pre>
<pre class="r"><code>## Complaints about slow response
# Base probability of a slow response complaint: 5%
prob_slow_response &lt;- rep(0.05, data.n)
# Email contacts: 6x higher likelihood
prob_slow_response[data$method == &quot;email&quot;] &lt;- prob_slow_response[data$method == &quot;email&quot;] * 6
# Chat and web_form: 3x higher likelihood
prob_slow_response[data$method %in% c(&quot;chat&quot;, &quot;web_form&quot;)] &lt;- prob_slow_response[data$method %in% c(&quot;chat&quot;, &quot;web_form&quot;)] * 3
# Ensure probability values remain within [0, 1]
prob_slow_response [prob_slow_response &gt; 1] &lt;- 1
prob_slow_response [prob_slow_response &lt; 0] &lt;- 0
# Generate binary variable
data$complain.sp.respond &lt;- rbinom(n = data.n, size = 1, prob = prob_slow_response)
# Check distribution across contact methods
with(data, prop.table(table(complain.sp.respond, method), margin = 2))</code></pre>
<pre><code>##                    method
## complain.sp.respond      chat     email     phone  web_form
##                   0 0.8513143 0.6948590 0.9489453 0.8577970
##                   1 0.1486857 0.3051410 0.0510547 0.1422030</code></pre>
<pre class="r"><code>## Complaints about slow resolution
# Base probability of a slow resolution complaint: 10%
prob_slow_resolution &lt;- rep(0.10, data.n)
# If contact is via chat and reason is incorrect_item → 5x higher likelihood
is_chat_incorrect &lt;- data$method == &quot;chat&quot; &amp; data$reason == &quot;incorrect_item&quot;
prob_slow_resolution[is_chat_incorrect] &lt;- prob_slow_resolution[is_chat_incorrect] * 5
# Ensure probability values remain within [0, 1]
prob_slow_resolution [prob_slow_resolution &gt; 1] &lt;- 1
prob_slow_resolution [prob_slow_resolution &lt; 0] &lt;- 0
# Generate binary variable
data$complain.sp.solve &lt;- rbinom(n = data.n, size = 1, prob = prob_slow_resolution)
# Check distribution by reason and method
with(data, prop.table(table(complain.sp.solve, reason, method), margin = c(2, 3)))</code></pre>
<pre><code>## , , method = chat
## 
##                  reason
## complain.sp.solve cancellation      delay incorrect_item      other     status
##                 0   0.90458716 0.89556300     0.50478838 0.87899687 0.90385812
##                 1   0.09541284 0.10443700     0.49521162 0.12100313 0.09614188
## 
## , , method = email
## 
##                  reason
## complain.sp.solve cancellation      delay incorrect_item      other     status
##                 0   0.91840278 0.90481400     0.90909091 0.90033784 0.88842975
##                 1   0.08159722 0.09518600     0.09090909 0.09966216 0.11157025
## 
## , , method = phone
## 
##                  reason
## complain.sp.solve cancellation      delay incorrect_item      other     status
##                 0   0.90989660 0.90199856     0.90151249 0.90321441 0.89479315
##                 1   0.09010340 0.09800144     0.09848751 0.09678559 0.10520685
## 
## , , method = web_form
## 
##                  reason
## complain.sp.solve cancellation      delay incorrect_item      other     status
##                 0   0.88755981 0.89090909     0.88353414 0.90306748 0.90326633
##                 1   0.11244019 0.10909091     0.11646586 0.09693252 0.09673367</code></pre>
<pre class="r"><code>## Complaints about having to repeat information
# Base probability of a repetition complaint: 2%
prob_repeat_info &lt;- rep(0.02, data.n)
# If contact reason is cancellation → 10x higher likelihood
prob_repeat_info[data$reason == &quot;cancellation&quot;] &lt;- prob_repeat_info[data$reason == &quot;cancellation&quot;] * 10
# Ensure probability values remain within [0, 1]
prob_repeat_info [prob_repeat_info &gt; 1] &lt;- 1
prob_repeat_info [prob_repeat_info &lt; 0] &lt;- 0
# Generate binary variable
data$complain.repeat &lt;- rbinom(n = data.n, size = 1, prob = prob_repeat_info)
# Check distribution across contact reasons
with(data, prop.table(table(complain.repeat, reason), margin = 2))</code></pre>
<pre><code>##                reason
## complain.repeat cancellation      delay incorrect_item      other     status
##               0   0.79010796 0.97868054     0.97952600 0.98004988 0.98118146
##               1   0.20989204 0.02131946     0.02047400 0.01995012 0.01881854</code></pre>
<pre class="r"><code>## Other types of complaints (uniform distribution)
# Constant probability of 20% for other unspecified complaints
data$complain.other &lt;- rbinom(n = data.n, size = 1, prob = 0.20)
# Check distribution
with(data, prop.table(table(complain.other)))</code></pre>
<pre><code>## complain.other
##       0       1 
## 0.79655 0.20345</code></pre>
</div>
<div id="simulation-of-nps-scores" class="section level2">
<h2>3.3. Simulation of NPS scores</h2>
<p>We simulate NPS scores by first assuming a normally distributed base
score in the ideal case where customers have no complaints.</p>
<p>Then, we apply fixed score penalties based on the presence of
different types of complaints.<br />
Finally, we constrain scores to the valid NPS range (0–10), convert them
to integers, and add placeholders for open-text responses.</p>
<pre class="r"><code>## Simulation of NPS scores

# Step 1: Simulate base NPS scores assuming no complaints (mean = 9, sd = 2)
data$nps.score &lt;- rnorm(data.n, mean = 9, sd = 2)

# Step 2: Apply fixed penalties based on complaints
# -3 for connection issues
# -4 for slow response
# -6 for slow resolution
# -2 for repeated information
# -1 for other complaints
data$nps.score &lt;- data$nps.score -
  3 * data$complain.connect -
  4 * data$complain.sp.respond -
  6 * data$complain.sp.solve -
  2 * data$complain.repeat -
  1 * data$complain.other 

# Step 3: Ensure scores stay within the 0–10 range
data$nps.score[data$nps.score &lt; 0] &lt;- 0
data$nps.score[data$nps.score &gt; 10] &lt;- 10 

# Step 4: Convert to integer values (using floor)
data$nps.score &lt;- floor(data$nps.score)

# Step 5: Add placeholder for open-text comments
data$open.comment &lt;- rep(&quot;bla bla&quot;, data.n)

# Step 6: Visualize score distribution
barplot(table(data$nps.score))</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>with(data, prop.table(table(data$nps.score)))</code></pre>
<pre><code>## 
##        0        1        2        3        4        5        6        7 
## 0.061800 0.031000 0.041925 0.053600 0.068450 0.083425 0.106050 0.125675 
##        8        9       10 
## 0.132175 0.119875 0.176025</code></pre>
<div id="specifying-the-nps-segments-based-on-nps-scores"
class="section level3">
<h3>3.3.1. Specifying the NPS segments based on NPS scores</h3>
<p>Based on standard NPS segmentation, we classify customers into three
categories: - <strong>Detractors</strong>: scores from 0 to 6 -
<strong>Passives</strong>: scores of 7 or 8 -
<strong>Promoters</strong>: scores of 9 or 10</p>
<p>We create a new categorical variable to reflect these groupings.</p>
<pre class="r"><code>## Create NPS segments based on score values

# Step 1: Initialize empty character variable
data$nps.segment &lt;- rep(NA_character_, data.n)

# Step 2: Assign segment based on score range
data$nps.segment[data$nps.score &lt;= 6] &lt;- &quot;detractor&quot;
data$nps.segment[data$nps.score &gt; 6 &amp; data$nps.score &lt; 9] &lt;- &quot;passive&quot;
data$nps.segment[data$nps.score &gt;= 9] &lt;- &quot;promoter&quot;

# Step 3: Convert to factor for categorical analysis
data$nps.segment &lt;- factor(data$nps.segment, levels = c(&quot;detractor&quot;, &quot;passive&quot;, &quot;promoter&quot;))

# Step 4: Check distribution across NPS segments
with(data, prop.table(table(nps.segment)))</code></pre>
<pre><code>## nps.segment
## detractor   passive  promoter 
##   0.44625   0.25785   0.29590</code></pre>
</div>
</div>
<div id="simulation-of-satisfaction-scores" class="section level2">
<h2>3.4. Simulation of satisfaction scores</h2>
<p>In addition to our main dependent variable (NPS), we simulate
satisfaction scores, which were available for some customer service
categories in the original project.</p>
<p>These scores will be useful to explore the <strong>convergent
validity</strong> of NPS, as they are expected to be positively
correlated.</p>
<pre class="r"><code>## Simulation of satisfaction scores (1 to 5 scale)

# Step 1: Generate satisfaction scores based on NPS
# We use a normal distribution centered at 3, with some variability (sd = 0.5)
# and add a scaled component based on NPS to simulate a positive relationship.
data$satisfaction &lt;- floor(
  rnorm(data.n, mean = 3, sd = 0.5) + 
  0.9 * (1 + as.numeric(scale(data$nps.score)))
)

# Step 2: Ensure scores stay within the 1–5 range
data$satisfaction[data$satisfaction &lt; 1] &lt;- 1
data$satisfaction[data$satisfaction &gt; 5] &lt;- 5

# Step 3: Visualize the distribution
barplot(table(data$satisfaction))</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>with(data, prop.table(table(satisfaction)))</code></pre>
<pre><code>## satisfaction
##       1       2       3       4       5 
## 0.04960 0.15300 0.28815 0.37000 0.13925</code></pre>
<pre class="r"><code># Step 4: Check correlation with NPS scores (Spearman method)
with(data, cor(nps.score, satisfaction, method = &quot;spearman&quot;))</code></pre>
<pre><code>## [1] 0.8293966</code></pre>
<div id="simulating-missing-data-for-satisfaction"
class="section level3">
<h3>3.4.1. Simulating missing data for satisfaction</h3>
<p>To reflect the restrictions observed in the original project, we
simulate that satisfaction data is <strong>not available</strong> in the
following conditions:</p>
<ul>
<li>For customers from <strong>Drinkland</strong></li>
<li>For contacts via <strong>email</strong> or <strong>web
form</strong></li>
</ul>
<p>In addition, we assume that even when satisfaction was supposed to be
collected, there is a <strong>10% probability of missingness</strong>,
simulating cases where customers simply skipped the question.</p>
<p>This missing data was a key consideration in the original project and
contributed to the decision to focus on likelihood to recommend (Net
Promoter Score) rather than satisfaction, as the latter was not
consistently available across segments.</p>
<pre class="r"><code>## Simulate missing data for satisfaction

# Step 1: Set satisfaction to missing for Drinkland
data$satisfaction[data$country == &quot;Drinkland&quot;] &lt;- NA_real_

# Step 2: Set satisfaction to missing for email and web_form contacts
data$satisfaction[data$method %in% c(&quot;email&quot;, &quot;web_form&quot;)] &lt;- NA_real_

# Step 3: Introduce 10% additional random missingness among remaining valid values
# Create a random vector (0 = missing, 1 = keep) for non-NA entries
na_randomizer &lt;- sample(
  c(0, 1),
  size = sum(!is.na(data$satisfaction)),
  replace = TRUE,
  prob = c(0.1, 0.9)
)
# Apply missing values where randomizer is 0
data$satisfaction[which(!is.na(data$satisfaction))[na_randomizer == 0]] &lt;- NA_real_

# Step 4: Check final distribution of satisfaction (including NAs)
with(data, prop.table(table(is.na(satisfaction), method, country), margin = 2:3))</code></pre>
<pre><code>## , , country = Drinkland
## 
##        method
##               chat      email      phone   web_form
##   FALSE 0.00000000 0.00000000 0.00000000 0.00000000
##   TRUE  1.00000000 1.00000000 1.00000000 1.00000000
## 
## , , country = Eatland
## 
##        method
##               chat      email      phone   web_form
##   FALSE 0.89319530 0.00000000 0.90619164 0.00000000
##   TRUE  0.10680470 1.00000000 0.09380836 1.00000000</code></pre>
</div>
</div>
</div>
<div id="exploratory-analysis-to-identify-key-areas-for-improvement"
class="section level1">
<h1>4. Exploratory Analysis to Identify Key Areas for Improvement</h1>
</div>
<div id="measurement-considerations-convergent-validity-of-nps-scores"
class="section level1">
<h1>4.1. Measurement Considerations: Convergent Validity of NPS
Scores</h1>
<p>The validity of our main dependent variable, the Net Promoter Score
(NPS), has been subject to criticism. Some of these critiques highlight
that NPS relies on a single item, which may not fully capture the
complexity of the likelihood to recommend. Additionally, the standard
NPS formulation specifically asks about recommending the brand to
friends or colleagues, which may not generalize to all recommendation
scenarios.</p>
<p>To build confidence in the use of NPS within our context, we examine
its convergent validity with satisfaction scores, which were available
for a subset of markets and contact methods. A strong positive
association between NPS and satisfaction would support the concurrent
validity of NPS as a proxy for overall customer evaluation.</p>
<div id="preliminary-considerations-for-correlations"
class="section level2">
<h2>4.1.1. Preliminary considerations for correlations</h2>
<p>To determine the most appropriate method for computing correlations,
we began by exploring the distributions of both satisfaction and NPS
scores. For this, we used bar charts generated with the ggplot2
package:</p>
<pre class="r"><code># Distribution of NPS scores
ggplot(data[!is.na(data$nps.score), ], aes(x = factor(nps.score))) +
  geom_bar(fill = &quot;#2980b9&quot;) +
  labs(
    title = &quot;Distribution of NPS Scores&quot;,
    x = &quot;NPS Score&quot;,
    y = &quot;Count&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code># Distribution of satisfaction scores
ggplot(data[!is.na(data$satisfaction), ], aes(x = factor(satisfaction))) +
  geom_bar(fill = &quot;#27ae60&quot;) +
  labs(
    title = &quot;Distribution of Satisfaction Scores&quot;,
    x = &quot;Satisfaction Score&quot;,
    y = &quot;Count&quot;
  ) +
  theme_minimal()</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<p>Neither nps.score nor satisfaction follows a normal distribution.
Both variables exhibit left-skewed patterns, with nps.score also showing
signs of a ceiling effect, as a substantial proportion of scores
accumulate near the top end of the scale.</p>
<p>Given these distributional characteristics, Pearson’s correlation is
not suitable—it assumes linearity, homoscedasticity, and approximately
normal distributions, and is highly sensitive to skewed or bounded
data.</p>
<p>To inform the selection of a more appropriate correlation method—such
as Spearman’s rank correlation or polychoric correlation—we first
inspect the relationship between nps.score and satisfaction visually. A
scatterplot allows us to evaluate whether the association appears
monotonic and to better understand its form and strength.</p>
<p>We use jittering to reduce overlap in the plot, given that both
variables are discrete and bounded, which can otherwise obscure the
pattern of relationships.</p>
<pre class="r"><code># Create a scatterplot to visualize the relationship between NPS and satisfaction

ggplot(na.omit(data[, c(&quot;nps.score&quot;, &quot;satisfaction&quot;)]), 
       aes(x = jitter(nps.score), y = jitter(satisfaction))) +
  geom_point(alpha = 0.5, color = &quot;#2c3e50&quot;) +  # Add semi-transparent points for visual clarity
  labs(
    title = &quot;Relationship between NPS Scores and Satisfaction&quot;,
    x = &quot;NPS Score&quot;,
    y = &quot;Satisfaction&quot;
  ) +
  theme_minimal()  </code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>The scatterplot reveals a strong, monotonic—and approximately
linear—relationship between NPS score and satisfaction.</p>
<div id="spearman-correlation-between-nps-and-satisfaction-scores"
class="section level3">
<h3>4.1.2. Spearman Correlation between NPS and Satisfaction Scores</h3>
<p>Given that both variables are ordinal in nature and not normally
distributed, we use Spearman’s rank correlation. This method evaluates
the strength and direction of a monotonic association, without assuming
interval-level measurement or normality.</p>
<pre class="r"><code># Compute Spearman&#39;s rank correlation between NPS and satisfaction using the &#39;psych&#39; package
with(data, corr.test(nps.score, satisfaction, method = &quot;spearman&quot;, use = &quot;complete.obs&quot;))</code></pre>
<pre><code>## Call:corr.test(x = nps.score, y = satisfaction, use = &quot;complete.obs&quot;, 
##     method = &quot;spearman&quot;)
## Correlation matrix 
## [1] 0.82
## Sample Size 
## [1] 13383
## These are the unadjusted probability values.
##   The probability values  adjusted for multiple tests are in the p.adj object. 
## [1] 0
## 
##  To see confidence intervals of the correlations, print with the short=FALSE option</code></pre>
<p>The results indicate a strong positive Spearman correlation between
satisfaction and NPS scores. This supports the convergent validity of
NPS in our dataset and suggests it is a meaningful proxy for overall
customer satisfaction.</p>
</div>
<div id="further-measurement-considerations" class="section level3">
<h3>4.1.3. Further Measurement Considerations</h3>
<p>Despite the evidence supporting the convergent validity of the
<strong>raw NPS scores</strong>, there remain concerns about the use of
the <strong>categorical NPS metric</strong>, which is calculated by
subtracting the percentage of detractors (scores 0–6) from the
percentage of promoters (scores 9–10).</p>
<p>This transformation introduces ambiguity for two reasons:<br />
1. <strong>Loss of information</strong>: Grouping continuous scores into
categories discards nuance.<br />
2. <strong>Distributional blindness</strong>: The categorical NPS does
not account for the full shape of the score distribution. For example,
an NPS of 20 could result from 20% promoters and 0% detractors—or from
60% promoters and 40% detractors—two very different satisfaction
profiles.</p>
<p>While we will use the categorical NPS in certain summaries and
visualizations—given its widespread familiarity among stakeholders, we
will systematically compare those results with analyses based on
<strong>raw NPS scores</strong>, to ensure consistency and
transparency.</p>
</div>
</div>
<div
id="preliminary-inspection-of-key-customer-service-factors-with-heatmaps"
class="section level2">
<h2>4.2. Preliminary Inspection of Key Customer Service Factors with
Heatmaps</h2>
<p>Given the large number of levels across the variables contact method,
contact reason, and country, it is important to <strong>explore
potential interactions</strong> between these factors before proceeding
to formal modeling.</p>
<p>It is crucial to consider these interactions in the modeling to avoid
confusions, such as attributing an effect to one factor when it is
actually driven by an interaction with another.</p>
<p>To visually inspect these relationships, we will use heatmaps to
examine the frequency of different combinations of these categorical
variables.</p>
<div id="reusable-function-to-generate-heatmaps" class="section level3">
<h3>4.2.1. Reusable Function to Generate Heatmaps</h3>
<p>We first define a function that generates summary heatmaps for any
dependent variable and any aggregation function we wish to apply (e.g.,
mean, categorical NPS, etc.), across our selected factors.</p>
<p>This function systematically provides results for all main effects as
well as all levels of interaction between the factors, facilitating a
comprehensive visual exploration of their relationships.</p>
<pre class="r"><code>f.heatmap.3f &lt;- function(my.data, my.dv, factor.columns, factor.rows, factor.blocks, my.function) {

  # Initialize list to store results and a counter
  results &lt;- list()
  k &lt;- 1

  # Extract unique levels of each factor
  factor.column.levels &lt;- unique(my.data[[factor.columns]])
  factor.rows.levels &lt;- unique(my.data[[factor.rows]])
  factor.blocks.levels &lt;- unique(my.data[[factor.blocks]])

  # 1. Overall results (no breakdown by block factor)

  # 1.1 First row: totals for whole dataset and totals by column factor
  row.label.t &lt;- &quot;Total all sample&quot;

  # Calculate overall summary metric (excluding NAs)
  vector.total &lt;- my.data[[my.dv]]
  vector.total.clean &lt;- vector.total[!is.na(vector.total)]
  if (length(vector.total.clean) &gt; 0) {
    total &lt;- my.function(vector.total.clean)
  } else {
    stop(&quot;The dataset is empty&quot;)
  }

  # Calculate summary metric for each level of the column factor
  c.total &lt;- sapply(factor.column.levels, function(c) {
    vector.c.total &lt;- my.data[my.data[[factor.columns]] == c, my.dv]
    vector.c.total.clean &lt;- vector.c.total[!is.na(vector.c.total)]
    if (length(vector.c.total.clean) &gt; 0) {
      my.function(vector.c.total.clean)
    } else {
      NA_real_
    }
  })

  # Save totals row in results list
  results[[k]] &lt;- c(group = row.label.t, overall = total, setNames(object = c.total, factor.column.levels))
  k &lt;- k + 1

  # 1.2 Additional rows: results for each level of the row factor,
  # with column factor breakdowns

  for (r in factor.rows.levels) {
    row.label.r &lt;- r

    r.total.vector &lt;- my.data[my.data[[factor.rows]] == r, my.dv]
    r.total.vector.clean &lt;- r.total.vector[!is.na(r.total.vector)]
    r.total &lt;- if (length(r.total.vector.clean) &gt; 0) my.function(r.total.vector.clean) else NA_real_

    rc.crossed &lt;- sapply(factor.column.levels, function(c) {
      rc.crossed.vector &lt;- my.data[
        my.data[[factor.rows]] == r &amp; my.data[[factor.columns]] == c,
        my.dv
      ]
      rc.crossed.vector.clean &lt;- rc.crossed.vector[!is.na(rc.crossed.vector)]
      if (length(rc.crossed.vector.clean) &gt; 0) {
        my.function(rc.crossed.vector.clean)
      } else {
        NA_real_
      }
    })

    results[[k]] &lt;- c(group = row.label.r, overall = r.total, setNames(object = rc.crossed, factor.column.levels))
    k &lt;- k + 1
  }

  # 2. Results broken down by block factor

  for (b in factor.blocks.levels) {
    row.label.b &lt;- paste0(&quot;Total: &quot;, b)

    b.vector &lt;- my.data[my.data[[factor.blocks]] == b, my.dv]
    b.vector.clean &lt;- b.vector[!is.na(b.vector)]
    b.total &lt;- if (length(b.vector.clean) &gt; 0) my.function(b.vector.clean) else NA_real_

    bc.crossed &lt;- sapply(factor.column.levels, function(c) {
      bc.crossed.vector &lt;- my.data[
        my.data[[factor.blocks]] == b &amp; my.data[[factor.columns]] == c,
        my.dv
      ]
      bc.crossed.vector.clean &lt;- bc.crossed.vector[!is.na(bc.crossed.vector)]
      if (length(bc.crossed.vector.clean) &gt; 0) my.function(bc.crossed.vector.clean) else NA_real_
    })

    results[[k]] &lt;- c(group = row.label.b, overall = b.total, setNames(object = bc.crossed, factor.column.levels))
    k &lt;- k + 1

    # 2.2 Rows within blocks: results for each row factor level within block,
    # with column factor breakdowns

    for (r in factor.rows.levels) {
      row.label.br &lt;- paste0(b, &quot;: &quot;, r)

      br.crossed.vector &lt;- my.data[
        my.data[[factor.blocks]] == b &amp; my.data[[factor.rows]] == r,
        my.dv
      ]
      br.crossed.vector.clean &lt;- br.crossed.vector[!is.na(br.crossed.vector)]
      br.crossed &lt;- if (length(br.crossed.vector.clean) &gt; 0) my.function(br.crossed.vector.clean) else NA_real_

      all.crossed &lt;- sapply(factor.column.levels, function(c) {
        all.crossed.vector &lt;- my.data[
          my.data[[factor.blocks]] == b &amp;
          my.data[[factor.rows]] == r &amp;
          my.data[[factor.columns]] == c,
          my.dv
        ]
        all.crossed.vector.clean &lt;- all.crossed.vector[!is.na(all.crossed.vector)]
        if (length(all.crossed.vector.clean) &gt; 0) my.function(all.crossed.vector.clean) else NA_real_
      })

      results[[k]] &lt;- c(group = row.label.br, overall = br.crossed, setNames(object = all.crossed, factor.column.levels))
      k &lt;- k + 1
    }
  }

  # 3. Combine results into a dataframe and convert numeric columns
  results.df &lt;- as.data.frame(do.call(rbind, results), stringsAsFactors = FALSE)
  results.df[, -1] &lt;- lapply(results.df[, -1], as.numeric)

  return(results.df)
}</code></pre>
</div>
<div id="heatmap-for-nps-in-categorical-scores" class="section level3">
<h3>4.2.2. Heatmap for NPS in Categorical Scores</h3>
<p>We first created a heatmap using NPS in its categorical form, as this
is the format most familiar to stakeholders. This choice facilitates
collaboration and interpretation, enabling domain experts to visually
identify potential patterns and effects across key service factors.</p>
<p>To do so, we implemented a simple, reusable function that computes
the categorical NPS score from any vector of raw NPS values. This
function includes checks for common data issues (e.g., missing or
out-of-range values) and provides warnings when the sample size may be
insufficient for stable estimates.</p>
<pre class="r"><code>f.nps.cat &lt;- function (my.nps.vector, na.rm=TRUE){ 
  # my.nps.vector is a numeric vector of raw NPS scores. It can contain NAs
  
  vector.c &lt;- my.nps.vector[!is.na(my.nps.vector)] # vector cleaned of NAs
  
  # Error messages if sample size = 0 or if there are out-of-range NPS values:
  if(length(vector.c) == 0) {stop(&quot;error: no available NPS raw scores&quot;)} 
  if(length(vector.c[vector.c &gt; 10]) &gt; 0) {stop(&quot;error: scores higher than 10 in the NPS raw scores&quot;)} 
  if(length(vector.c[vector.c &lt; 0]) &gt; 0) {stop(&quot;error: scores lower than 0 in the NPS raw scores&quot;)} 
  
  # Warning if sample size is below 70 (low reliability)
  if(length(vector.c) &lt; 70) {warning(&quot;few scores available for calculation (n&lt;70)&quot;)} 
  
  n &lt;- length(vector.c) # sample size (excluding NAs)
  prop.promoters &lt;- (length(vector.c[vector.c &gt;= 9])) / n # proportion of promoters
  prop.detractors &lt;- (length(vector.c[vector.c &lt;= 6])) / n # proportion of detractors
  nps.cat &lt;- (prop.promoters - prop.detractors) * 100 # NPS score in categorical terms
  
  return(nps.cat) # returns NPS as a categorical score
}

# Check if it is working as expected
f.nps.cat(data$nps.score)</code></pre>
<pre><code>## [1] -15.035</code></pre>
<pre class="r"><code>with(data, prop.table(table(nps.segment)))</code></pre>
<pre><code>## nps.segment
## detractor   passive  promoter 
##   0.44625   0.25785   0.29590</code></pre>
<p>The function is working correctly (we can confirm that the result
matches the subtraction of the percentage of promoters and
detractors).</p>
<p>We are now ready to generate the heatmap for the categorical NPS
using the reusable functions defined earlier.</p>
<pre class="r"><code># Use the function to calculate heatmaps
nps.cat.heatmap &lt;- f.heatmap.3f(
  my.data = data,
  my.dv = &quot;nps.score&quot;,
  factor.columns = &quot;method&quot;,
  factor.rows = &quot;reason&quot;,
  factor.blocks = &quot;country&quot;,
  my.function = f.nps.cat  # Function to compute categorical NPS scores
)

# Export the results to Excel to apply conditional formatting (e.g., color scales)
write.xlsx(nps.cat.heatmap, &quot;nps.cat.heatmap.xlsx&quot;, colnames = TRUE)

#Check if you have acceptable sample sizes for all cells, using also the heatmap function
nps.cat.heatmap.n &lt;- f.heatmap.3f (
  my.data = data,
  my.dv = &quot;nps.score&quot;,
  factor.columns = &quot;method&quot;,
  factor.rows = &quot;reason&quot;,
  factor.blocks = &quot;country&quot;,
  my.function = length
)</code></pre>
<p>All sample sizes across the heatmap cells are sufficiently large (n
&gt; 150), reducing the likelihood of sampling error and increasing
confidence in the observed patterns.</p>
<p>After applying conditional formatting (color scales) in Excel, we
obtain the heatmap, which helps us identify areas where the probability
of recommending our customer service is particularly low. This
visualization allows us to assess whether these effects are driven by
main factors or by interactions between them.</p>
<pre class="r"><code># Import the image of the heatmap generated in Excel with conditional formatting
knitr::include_graphics(&quot;Heatmap.nps.cat.png&quot;)</code></pre>
<p><img src="Heatmap.nps.cat.png" /><!-- --></p>
<p>The heatmap reveals several <strong>main effects</strong>:</p>
<ul>
<li><p>Customers in Drinkland show a lower probability to promote
compared to those in Eatland (see the yellow-colored area in the
Drinkland block).</p></li>
<li><p>Customers contacting for cancellations are less likely to promote
than those contacting for other reasons (see the redder rows
corresponding to cancellation reasons).</p></li>
<li><p>Phone is associated with the highest probability to promote,
while email shows the lowest (see the greener column for phone and the
redder column for email).</p></li>
</ul>
<p>More importantly, the heatmap helps us detect an
<strong>interaction</strong> between contact reason and contact method:
Customers who contact through chat for incorrect item deliveries show a
particularly low probability to promote (see the deep red cells at the
intersection of the chat column and incorrect item row).</p>
<p>Before proceeding to statistically model these effects, we will check
whether similar patterns emerge when using alternative outcome measures,
such as raw NPS scores and satisfaction ratings.</p>
</div>
<div id="heatmap-for-nps-raw-scores" class="section level3">
<h3>4.2.3. Heatmap for NPS Raw Scores</h3>
<p>To ensure that the limited categorization in the NPS categorical
scores does not lead to misleading patterns or ambiguities, we examine
whether similar patterns emerge when using the NPS raw scores. For this,
we apply our reusable heatmap function to calculate the mean of the NPS
raw scores across the factors.</p>
<pre class="r"><code># Apply the heatmap function to NPS raw scores
nps.raw.heatmap &lt;- f.heatmap.3f(
  my.data = data,
  my.dv = &quot;nps.score&quot;,
  factor.columns = &quot;method&quot;,
  factor.rows = &quot;reason&quot;,
  factor.blocks = &quot;country&quot;,
  my.function = mean  # Calculation of the mean of the NPS raw scores
)

# Export the heatmap data to Excel for conditional formatting (color scales)
write.xlsx(nps.raw.heatmap, &quot;nps.raw.heatmap.xlsx&quot;, colnames = TRUE)

# Visualize the heatmap after adding colors in Excel
knitr::include_graphics(&quot;Heatmap.nps.2.png&quot;)</code></pre>
<p><img src="Heatmap.nps.2.png" /><!-- --></p>
<p>We observe a similar pattern in both methods of calculating NPS
results. However, it is also valuable to examine whether satisfaction
scores reveal comparable patterns.</p>
</div>
<div id="heatmap-for-satisfaction-scores" class="section level3">
<h3>4.2.4. Heatmap for Satisfaction Scores</h3>
<p>As further evidence of convergent validity between NPS and
satisfaction scores, we assess whether these two measures show similar
patterns across our factors.</p>
<p>To do this, we apply our reusable heatmap function to compute the
mean satisfaction scores.</p>
<pre class="r"><code># Apply the heatmap function to satisfaction scores
satisfaction.heatmap &lt;- f.heatmap.3f(
  my.data = data,
  my.dv = &quot;satisfaction&quot;,
  factor.columns = &quot;method&quot;,
  factor.rows = &quot;reason&quot;,
  factor.blocks = &quot;country&quot;,
  my.function = mean  # Calculation of the mean satisfaction scores
)

# Export the heatmap data to Excel for conditional formatting (color scales)
write.xlsx(satisfaction.heatmap, &quot;satisfaction.heatmap.xlsx&quot;, colnames = TRUE)

# Visualize all heatmaps after applying colors in Excel
knitr::include_graphics(&quot;Heatmap.png&quot;)</code></pre>
<p><img src="Heatmap.png" /><!-- --></p>
<p>We observe a similar pattern for satisfaction and both methods of
calculating NPS results, which increases our confidence in the validity
of NPS scores within this context.</p>
</div>
<div id="correlations-between-results-of-different-dependent-variables"
class="section level3">
<h3>4.2.5. Correlations between Results of Different Dependent
Variables</h3>
<p>To further assess convergence between the dependent variables, we
calculate Pearson correlations between their heatmap results.</p>
<p>First, we reshape each heatmap dataframe into a long format with a
single column for the values. Then, we merge these datasets and
calculate the correlations.</p>
<pre class="r"><code># Reshape heatmap results into long format
sat.long &lt;- satisfaction.heatmap %&gt;% pivot_longer(cols = -1,
                                 names_to = &quot;evaluation&quot;,
                                 values_to = &quot;satisfaction&quot;)

nps.raw.long &lt;- nps.raw.heatmap %&gt;% pivot_longer(cols = -1,
                                 names_to = &quot;evaluation&quot;,
                                 values_to = &quot;nps.raw&quot;)

nps.cat.long &lt;- nps.cat.heatmap %&gt;% pivot_longer(cols = -1,
                                 names_to = &quot;evaluation&quot;,
                                 values_to = &quot;nps.cat&quot;)

# Merge all heatmap results into a single dataset
merged.heatmaps &lt;- merge(nps.cat.long, merge(sat.long, nps.raw.long, by = c(&quot;group&quot;, &quot;evaluation&quot;)), by = c(&quot;group&quot;, &quot;evaluation&quot;))

# Calculate Pearson correlations across the three measures
cor(merged.heatmaps[, 3:5], use = &quot;complete.obs&quot;)</code></pre>
<pre><code>##                nps.cat satisfaction   nps.raw
## nps.cat      1.0000000    0.9693055 0.9867490
## satisfaction 0.9693055    1.0000000 0.9831536
## nps.raw      0.9867490    0.9831536 1.0000000</code></pre>
<p>All three measures show strong correlations (r &gt; 0.90), providing
additional confidence that the NPS is a valid measure and that important
patterns are not missed by relying on any single metric.</p>
</div>
</div>
<div
id="modeling-key-factors-associated-with-the-probability-to-promote-the-brand"
class="section level2">
<h2>4.3. Modeling Key Factors Associated with the Probability to Promote
the Brand</h2>
<p>While the heatmaps provided valuable visual insights into areas where
customer service performance appears to be lower, statistical modeling
is necessary to formally quantify these relationships.</p>
<div id="setting-reference-levels-for-factor-categories"
class="section level3">
<h3>4.3.1. Setting Reference Levels for Factor Categories</h3>
<p>To make the interpretation of model coefficients more intuitive, we
set the reference levels of our categorical predictors based on the
heatmap results. Specifically, we use the levels associated with higher
NPS scores as the reference. This allows us to interpret model outputs
as comparisons against the best-performing category for each factor.</p>
<pre class="r"><code># Set the order of the factor levels
data$method &lt;- factor(data$method, levels = c(&quot;phone&quot;, &quot;web_form&quot;, &quot;chat&quot;, &quot;email&quot;))
data$reason &lt;- factor(data$reason, levels = c(&quot;other&quot;,&quot;status&quot;, &quot;delay&quot;, &quot;cancellation&quot;, &quot;incorrect_item&quot;))
data$country &lt;- factor(data$country, levels = c(&quot;Eatland&quot;, &quot;Drinkland&quot;))</code></pre>
</div>
<div id="discarding-a-linear-regression-model." class="section level3">
<h3>4.3.2. Discarding a Linear Regression Model.</h3>
<p>We discard the linear model because our outcome variable, NPS score,
is ordinal in nature, and it is difficult to assume equal distances
between adjacent scores given the ceiling and floor effects observed in
its distribution (see Section 4.1.1).</p>
<p>In addition, a linear model fails to meet key assumptions like the
normality of residuals, as shown in the Q-Q plots below, even when we
apply a square transformation to the dependent variable.</p>
<pre class="r"><code>#Set the configuration to see 4 plots in the same window: 
par(mfrow = c(2, 2)) 

# Check normality and homoscedasticity of residuals for the linear model
linear.model &lt;- lm(nps.score ~ method + reason + country, data = data)
plot(linear.model)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<pre class="r"><code># Check the same for a squared transformation of the dependent variable
linear.model.sq &lt;- lm((nps.score^2) ~ method + reason + country, data = data)
plot(linear.model.sq)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-19-2.png" width="672" /></p>
<p>As shown in the Q-Q plots, the distribution of residuals deviates
substantially from the theoretical normal distribution.</p>
</div>
<div id="evaluating-the-appropriateness-of-an-ordinal-model"
class="section level3">
<h3>4.3.3. Evaluating the Appropriateness of an Ordinal Model</h3>
<p>We prioritize testing an ordinal logistic model over a binomial
logistic model, as it allows us to estimate the effects of our
predictors across the full NPS scale, rather than focusing on a single
outcome category (e.g., detractors).</p>
<p>However, before proceeding, we need to check whether key assumptions
of this model are met—particularly the absence of multicollinearity and
the proportional odds assumption across the levels of our dependent
variable.</p>
<div id="checking-the-collinearity-assumption" class="section level4">
<h4>4.3.3.1. Checking the Collinearity Assumption</h4>
<p>We assess potential collinearity among our categorical predictors
using both visualizations and statistical diagnostics.</p>
<pre class="r"><code># Mosaic plot to visualize the relationships between categorical factors (vcd package)
doubledecker(with(data, table(country, reason, method)))</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code># VIF scores from the linear model previously computed (car package)
vif(linear.model)</code></pre>
<pre><code>##             GVIF Df GVIF^(1/(2*Df))
## method  1.000321  3        1.000054
## reason  1.000397  4        1.000050
## country 1.000188  1        1.000094</code></pre>
<p>The VIF analysis shows no indication of multicollinearity among the
predictors. Specifically, the adjustment of the Generalized Variance
Inflation Factors (GVIF^(1/(2*Df))), which allows for the comparison of
categorical variables with different degrees of freedom, yields values
that are all very close to 1.0.</p>
<p>This statistical result is further supported by the mosaic plot,
which shows no strong associations between the levels of the factors.
The distribution of frequencies appears relatively uniform across these
levels.</p>
<div id="checking-the-proportional-odds-assumption"
class="section level5">
<h5>4.3.3.2. Checking the Proportional Odds Assumption</h5>
<p>Before testing the proportional odds assumption, we create dummy
variables for the factors with multiple categories. This allows us to
assess the assumption separately for each category.</p>
<p>We also ensure that our dependent variable, the NPS score, is
properly defined as an ordinal factor.</p>
<pre class="r"><code># Dummy variables for method (reference: phone)
data$chat &lt;- ifelse(data$method == &quot;chat&quot;, 1, 0)
data$email &lt;- ifelse(data$method == &quot;email&quot;, 1, 0)
data$web_form &lt;- ifelse(data$method == &quot;web_form&quot;, 1, 0)

# Dummy variables for reason (reference: other)
data$delay &lt;- ifelse(data$reason == &quot;delay&quot;, 1, 0)
data$cancellation &lt;- ifelse(data$reason == &quot;cancellation&quot;, 1, 0)
data$incorrect_item &lt;- ifelse(data$reason == &quot;incorrect_item&quot;, 1, 0)
data$status &lt;- ifelse(data$reason == &quot;status&quot;, 1, 0)

# Register the NPS score as an ordinal factor
data$nps.score &lt;- factor(data$nps.score, levels = 0:10, ordered = TRUE)</code></pre>
<p>We now test the proportional odds assumption using the ordinal
package.</p>
<pre class="r"><code># Specify the ordinal logistic regression model with main effects
ordinal.model &lt;- clm(nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item + country, data = data)

# Summary of the model
summary(ordinal.model)</code></pre>
<pre><code>## formula: 
## nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item + country
## data:    data
## 
##  link  threshold nobs  logLik    AIC       niter max.grad cond.H 
##  logit flexible  40000 -90899.01 181834.03 7(0)  4.32e-12 5.6e+02
## 
## Coefficients:
##                   Estimate Std. Error z value Pr(&gt;|z|)    
## web_form         -0.201977   0.031405  -6.431 1.26e-10 ***
## chat             -0.460248   0.020263 -22.713  &lt; 2e-16 ***
## email            -0.493613   0.027018 -18.270  &lt; 2e-16 ***
## delay             0.005406   0.025112   0.215    0.830    
## status            0.037223   0.027441   1.356    0.175    
## cancellation     -0.210165   0.033712  -6.234 4.54e-10 ***
## incorrect_item   -0.449426   0.027844 -16.141  &lt; 2e-16 ***
## countryDrinkland -0.155422   0.017474  -8.895  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Threshold coefficients:
##      Estimate Std. Error  z value
## 0|1  -3.21679    0.03170 -101.467
## 1|2  -2.77176    0.02942  -94.204
## 2|3  -2.34536    0.02787  -84.155
## 3|4  -1.93914    0.02680  -72.358
## 4|5  -1.53253    0.02601  -58.920
## 5|6  -1.12321    0.02544  -44.159
## 6|7  -0.66723    0.02502  -26.667
## 7|8  -0.15237    0.02483   -6.138
## 8|9   0.43336    0.02498   17.346
## 9|10  1.11668    0.02585   43.192</code></pre>
<pre class="r"><code># Test of the proportional odds assumption
nominal_test(ordinal.model)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["logLik"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["AIC"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["LRT"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Pr(>Chi)"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"NA","2":"-90899.01","3":"181834.0","4":"NA","5":"NA","_rn_":"<none>"},{"1":"9","2":"-90894.43","3":"181842.9","4":"9.17022","5":"4.217122e-01","_rn_":"web_form"},{"1":"9","2":"-90858.90","3":"181771.8","4":"80.23498","5":"1.451423e-13","_rn_":"chat"},{"1":"9","2":"-90885.82","3":"181825.6","4":"26.39741","5":"1.758324e-03","_rn_":"email"},{"1":"9","2":"-90887.99","3":"181830.0","4":"22.05053","5":"8.720203e-03","_rn_":"delay"},{"1":"9","2":"-90889.19","3":"181832.4","4":"19.64169","5":"2.025713e-02","_rn_":"status"},{"1":"9","2":"-90891.85","3":"181837.7","4":"14.33103","5":"1.110276e-01","_rn_":"cancellation"},{"1":"9","2":"-90819.25","3":"181692.5","4":"159.52009","5":"9.343094e-30","_rn_":"incorrect_item"},{"1":"9","2":"-90888.47","3":"181830.9","4":"21.08388","5":"1.228316e-02","_rn_":"country"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The results suggest that the proportional odds assumption is violated
for most of the predictors (all except contacting via web form and
contacting for cancellation issues). This implies that the effect of
these predictors varies depending on which threshold along the NPS scale
is considered.</p>
<p>We will inspect several ways to solve this issue.</p>
<div
id="addressing-non-proportional-odds-by-incorporating-an-interaction-term"
class="section level6">
<h6>4.3.3.2.1 Addressing Non-Proportional Odds by Incorporating an
Interaction Term</h6>
<p>To address the observed violation of the proportional odds
assumption, we explore the inclusion of a theoretically meaningful
interaction in the model.</p>
<p>In the heatmaps, we identified a strong interaction: customers who
contacted customer service via chat regarding incorrect item deliveries
showed particularly low NPS scores. Including this interaction in the
model may not only improve interpretability but also help address the
previously observed violations of the proportional odds assumption for
the method and reason factors.</p>
<p>We begin by evaluating whether this model that includes the
interaction term offers a more parsimonious and appropriate fit:</p>
<pre class="r"><code># Step 1: Create a binary variable for the interaction between &#39;chat&#39; and &#39;incorrect_item&#39;
data$chat_incorrect_item &lt;- NA_real_
data$chat_incorrect_item[data$method == &quot;chat&quot; &amp; data$reason == &quot;incorrect_item&quot;] &lt;- 1
data$chat_incorrect_item[!(data$method == &quot;chat&quot; &amp; data$reason == &quot;incorrect_item&quot;)] &lt;- 0

# Check that the variable has been created correctly
with(data, table(chat_incorrect_item, method, reason))</code></pre>
<pre><code>## , , reason = other
## 
##                    method
## chat_incorrect_item phone web_form chat email
##                   0  2831      815 3190  1184
##                   1     0        0    0     0
## 
## , , reason = status
## 
##                    method
## chat_incorrect_item phone web_form chat email
##                   0  2804      796 3214  1210
##                   1     0        0    0     0
## 
## , , reason = delay
## 
##                    method
## chat_incorrect_item phone web_form chat email
##                   0  4153     1155 4778  1828
##                   1     0        0    0     0
## 
## , , reason = cancellation
## 
##                    method
## chat_incorrect_item phone web_form chat email
##                   0  1354      418 1635   576
##                   1     0        0    0     0
## 
## , , reason = incorrect_item
## 
##                    method
## chat_incorrect_item phone web_form chat email
##                   0  2843      747    0  1232
##                   1     0        0 3237     0</code></pre>
<pre class="r"><code># Step 2: Build the ordinal logistic model including the interaction term
ordinal.model.int &lt;- clm(nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item + country + chat_incorrect_item, data = data)

# Review model summary
summary(ordinal.model.int)</code></pre>
<pre><code>## formula: 
## nps.score ~ web_form + chat + email + delay + status + cancellation + incorrect_item + country + chat_incorrect_item
## data:    data
## 
##  link  threshold nobs  logLik    AIC       niter max.grad cond.H 
##  logit flexible  40000 -90577.31 181192.62 7(0)  7.22e-12 5.6e+02
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## web_form            -0.197043   0.031418  -6.272 3.57e-10 ***
## chat                -0.242901   0.021997 -11.043  &lt; 2e-16 ***
## email               -0.497631   0.027037 -18.405  &lt; 2e-16 ***
## delay                0.004497   0.025141   0.179    0.858    
## status               0.035698   0.027478   1.299    0.194    
## cancellation        -0.215385   0.033768  -6.378 1.79e-10 ***
## incorrect_item      -0.018957   0.032589  -0.582    0.561    
## countryDrinkland    -0.156083   0.017471  -8.934  &lt; 2e-16 ***
## chat_incorrect_item -1.157258   0.045684 -25.332  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Threshold coefficients:
##      Estimate Std. Error z value
## 0|1  -3.17140    0.03191 -99.385
## 1|2  -2.72022    0.02962 -91.832
## 2|3  -2.28646    0.02806 -81.491
## 3|4  -1.87243    0.02698 -69.394
## 4|5  -1.45807    0.02620 -55.651
## 5|6  -1.04174    0.02565 -40.621
## 6|7  -0.57980    0.02526 -22.953
## 7|8  -0.06078    0.02510  -2.422
## 8|9   0.52741    0.02528  20.859
## 9|10  1.21201    0.02617  46.313</code></pre>
<pre class="r"><code># Step 3: Compare the models with and without the interaction
anova(ordinal.model, ordinal.model.int)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["no.par"],"name":[1],"type":["int"],"align":["right"]},{"label":["AIC"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["logLik"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["LR.stat"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["df"],"name":[5],"type":["int"],"align":["right"]},{"label":["Pr(>Chisq)"],"name":[6],"type":["dbl"],"align":["right"]}],"data":[{"1":"18","2":"181834.0","3":"-90899.01","4":"NA","5":"NA","6":"NA","_rn_":"ordinal.model"},{"1":"19","2":"181192.6","3":"-90577.31","4":"643.4046","5":"1","6":"6.074202e-142","_rn_":"ordinal.model.int"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The model shows that the interaction is significant, and the
likelihood ratio test indicates a statistically significant improvement
in model fit after incorporating the interaction (chi-squared test p
&lt; .001 and a lower AIC of 181193 compared to 181834 for the simpler
model).</p>
<p>Next, we proceed to check the proportional odds assumption for the
updated model:</p>
<pre class="r"><code># Check the proportional odds assumption using the ordinal package
nominal_test(ordinal.model.int)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["logLik"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["AIC"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["LRT"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Pr(>Chi)"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"NA","2":"-90577.31","3":"181192.6","4":"NA","5":"NA","_rn_":"<none>"},{"1":"9","2":"-90573.95","3":"181203.9","4":"6.719087","5":"6.663400e-01","_rn_":"web_form"},{"1":"9","2":"-90559.43","3":"181174.9","4":"35.754413","5":"4.382805e-05","_rn_":"chat"},{"1":"9","2":"-90563.09","3":"181182.2","4":"28.447262","5":"8.027757e-04","_rn_":"email"},{"1":"9","2":"-90572.36","3":"181200.7","4":"9.899109","5":"3.587145e-01","_rn_":"delay"},{"1":"9","2":"-90571.83","3":"181199.7","4":"10.965216","5":"2.781007e-01","_rn_":"status"},{"1":"9","2":"-90571.83","3":"181199.7","4":"10.972194","5":"2.776197e-01","_rn_":"cancellation"},{"1":"9","2":"-90544.56","3":"181145.1","4":"65.505841","5":"1.150132e-10","_rn_":"incorrect_item"},{"1":"9","2":"-90566.15","3":"181188.3","4":"22.319468","5":"7.919793e-03","_rn_":"country"},{"1":"9","2":"-90467.96","3":"180991.9","4":"218.702764","5":"3.922235e-42","_rn_":"chat_incorrect_item"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>After including the interaction, the proportional odds assumption is
now satisfied for the contact reason delay. However, it remains violated
for most other factors.</p>
<p>To address this, we will explore a model that allows coefficients to
vary across thresholds for predictors that violate the assumption.</p>
</div>
<div
id="addressing-non-proportional-odds-by-modeling-varying-coefficients"
class="section level6">
<h6>4.3.3.2.2 Addressing Non-Proportional Odds by Modeling Varying
Coefficients</h6>
<p>We fit a model that relaxes the proportional odds assumption for
selected predictors, allowing their effects to differ at each NPS score
interval, using the VGAM package.</p>
<p>Since delay met the proportional odds assumption and showed no
significant effect, we exclude it from this model.</p>
<pre class="r"><code># Model allowing some predictors to have non-proportional effects
ordinal.model.int.non_proportional &lt;- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form),
  data = data
)</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in eval(slot(family, &quot;deriv&quot;)): some probabilities are very close to 0</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1
## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :
## iterations terminated because half-step sizes are very small</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some
## quantities such as z, residuals, SEs may be inaccurate due to convergence at a
## half-step</code></pre>
<pre class="r"><code>summary(ordinal.model.int.non_proportional)</code></pre>
<pre><code>## Warning in eval(expr): some probabilities are very close to 0</code></pre>
<pre><code>## 
## Call:
## vglm(formula = nps.score ~ web_form + chat + email + status + 
##     cancellation + incorrect_item + country + chat_incorrect_item, 
##     family = cumulative(link = logitlink, parallel = ~country + 
##         web_form), data = data)
## 
## Coefficients: 
##                        Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -0.42190    0.02160 -19.536  &lt; 2e-16 ***
## web_form                0.19960    0.03454   5.779 7.53e-09 ***
## chat:1                 -1.93165    0.03943 -48.986  &lt; 2e-16 ***
## chat:2                 -1.56548    0.03472 -45.083  &lt; 2e-16 ***
## chat:3                 -1.21348    0.03125 -38.835  &lt; 2e-16 ***
## chat:4                 -0.89505    0.02882 -31.057  &lt; 2e-16 ***
## chat:5                 -0.57340    0.02701 -21.228  &lt; 2e-16 ***
## chat:6                 -0.25066    0.02579  -9.719  &lt; 2e-16 ***
## chat:7                  0.11700    0.02508   4.666 3.07e-06 ***
## chat:8                  0.53194    0.02507  21.216  &lt; 2e-16 ***
## chat:9                  0.98845    0.02606  37.931  &lt; 2e-16 ***
## chat:10                 1.52202    0.02861  53.206  &lt; 2e-16 ***
## email:1                -1.28375    0.04593 -27.953  &lt; 2e-16 ***
## email:2                -0.95743    0.04078 -23.478  &lt; 2e-16 ***
## email:3                -0.66537    0.03707 -17.947  &lt; 2e-16 ***
## email:4                -0.39665    0.03432 -11.556  &lt; 2e-16 ***
## email:5                -0.08349    0.03202  -2.608 0.009115 ** 
## email:6                 0.16013    0.03085   5.192 2.09e-07 ***
## email:7                 0.42071    0.03038  13.848  &lt; 2e-16 ***
## email:8                 0.69152    0.03089  22.384  &lt; 2e-16 ***
## email:9                 1.03677    0.03296  31.458  &lt; 2e-16 ***
## email:10                1.45832    0.03727  39.127  &lt; 2e-16 ***
## status:1               -1.66305    0.04467 -37.231  &lt; 2e-16 ***
## status:2               -1.41002    0.03968 -35.531  &lt; 2e-16 ***
## status:3               -1.17635    0.03576 -32.898  &lt; 2e-16 ***
## status:4               -0.92717    0.03234 -28.671  &lt; 2e-16 ***
## status:5               -0.66468    0.02952 -22.515  &lt; 2e-16 ***
## status:6               -0.40838    0.02760 -14.799  &lt; 2e-16 ***
## status:7               -0.13678    0.02644  -5.173 2.30e-07 ***
## status:8                0.18467    0.02628   7.027 2.12e-12 ***
## status:9                0.55337    0.02754  20.091  &lt; 2e-16 ***
## status:10               0.94569    0.03035  31.156  &lt; 2e-16 ***
## cancellation:1         -1.40564    0.05638 -24.931  &lt; 2e-16 ***
## cancellation:2         -1.15036    0.05008 -22.969  &lt; 2e-16 ***
## cancellation:3         -0.94113    0.04561 -20.633  &lt; 2e-16 ***
## cancellation:4         -0.67012    0.04102 -16.335  &lt; 2e-16 ***
## cancellation:5         -0.40205    0.03757 -10.700  &lt; 2e-16 ***
## cancellation:6         -0.13056    0.03534  -3.694 0.000221 ***
## cancellation:7          0.15890    0.03438   4.622 3.80e-06 ***
## cancellation:8          0.43331    0.03497  12.392  &lt; 2e-16 ***
## cancellation:9          0.75349    0.03744  20.124  &lt; 2e-16 ***
## cancellation:10         1.15615    0.04266  27.103  &lt; 2e-16 ***
## incorrect_item:1       -2.34455    0.06492 -36.116  &lt; 2e-16 ***
## incorrect_item:2       -1.94980    0.05477 -35.598  &lt; 2e-16 ***
## incorrect_item:3       -1.58866    0.04750 -33.448  &lt; 2e-16 ***
## incorrect_item:4       -1.22032    0.04181 -29.188  &lt; 2e-16 ***
## incorrect_item:5       -0.89431    0.03803 -23.515  &lt; 2e-16 ***
## incorrect_item:6       -0.55887    0.03541 -15.783  &lt; 2e-16 ***
## incorrect_item:7       -0.13328    0.03365  -3.961 7.46e-05 ***
## incorrect_item:8        0.34171    0.03346  10.212  &lt; 2e-16 ***
## incorrect_item:9        0.86785    0.03528  24.601  &lt; 2e-16 ***
## incorrect_item:10       1.42920    0.03959  36.096  &lt; 2e-16 ***
## countryDrinkland        0.15288    0.01811   8.441  &lt; 2e-16 ***
## chat_incorrect_item:1   4.04837    0.08202  49.361  &lt; 2e-16 ***
## chat_incorrect_item:2   3.63775    0.07123  51.068  &lt; 2e-16 ***
## chat_incorrect_item:3   3.18739    0.06391  49.869  &lt; 2e-16 ***
## chat_incorrect_item:4   2.68196    0.05870  45.692  &lt; 2e-16 ***
## chat_incorrect_item:5   2.17923    0.05539  39.340  &lt; 2e-16 ***
## chat_incorrect_item:6   1.66879    0.05341  31.244  &lt; 2e-16 ***
## chat_incorrect_item:7   1.05517    0.05260  20.061  &lt; 2e-16 ***
## chat_incorrect_item:8   0.47888    0.05423   8.830  &lt; 2e-16 ***
## chat_incorrect_item:9  -0.04240    0.05976  -0.709 0.478027    
## chat_incorrect_item:10 -0.56450    0.07099  -7.952 1.83e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Number of linear predictors:  10 
## 
## Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2]), 
## logitlink(P[Y&lt;=3]), logitlink(P[Y&lt;=4]), logitlink(P[Y&lt;=5]), logitlink(P[Y&lt;=6]), 
## logitlink(P[Y&lt;=7]), logitlink(P[Y&lt;=8]), logitlink(P[Y&lt;=9]), logitlink(P[Y&lt;=10])
## 
## Residual deviance: 4942097 on 399937 degrees of freedom
## 
## Log-likelihood: NA on 399937 degrees of freedom
## 
## Number of Fisher scoring iterations: 2 
## 
## Warning: Hauck-Donner effect detected in the following estimate(s):
## &#39;incorrect_item:1&#39;</code></pre>
<p>The previous model did not converge properly and generated warnings
about over-predicting certain outcomes, resulting in unstable parameter
estimates.</p>
<p>To address this, we fit a simpler model that only relaxes the
proportional odds assumption for the predictors with the strongest
violations: the contact reason incorrect item, and the interaction term
of incorrect items attended by chat.</p>
<pre class="r"><code># Model with fewer predictors allowed to have non-proportional effects
ordinal.model.int.non_proportional2 &lt;- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form + chat + email + status + cancellation),
  data = data
)</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in eval(slot(family, &quot;deriv&quot;)): some probabilities are very close to 0</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1
## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :
## iterations terminated because half-step sizes are very small</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some
## quantities such as z, residuals, SEs may be inaccurate due to convergence at a
## half-step</code></pre>
<pre class="r"><code>summary(ordinal.model.int.non_proportional2)</code></pre>
<pre><code>## Warning in eval(expr): some probabilities are very close to 0</code></pre>
<pre><code>## 
## Call:
## vglm(formula = nps.score ~ web_form + chat + email + status + 
##     cancellation + incorrect_item + country + chat_incorrect_item, 
##     family = cumulative(link = logitlink, parallel = ~country + 
##         web_form + chat + email + status + cancellation), data = data)
## 
## Coefficients: 
##                        Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -0.42190    0.02144 -19.673  &lt; 2e-16 ***
## web_form                0.19959    0.03506   5.693 1.25e-08 ***
## chat                    0.24346    0.02536   9.600  &lt; 2e-16 ***
## email                   0.49162    0.02984  16.473  &lt; 2e-16 ***
## status                 -0.03397    0.02672  -1.271    0.204    
## cancellation            0.21551    0.03491   6.174 6.67e-10 ***
## incorrect_item:1       -2.79815    0.06881 -40.666  &lt; 2e-16 ***
## incorrect_item:2       -2.32003    0.05670 -40.920  &lt; 2e-16 ***
## incorrect_item:3       -1.88427    0.04841 -38.921  &lt; 2e-16 ***
## incorrect_item:4       -1.44728    0.04227 -34.238  &lt; 2e-16 ***
## incorrect_item:5       -1.04125    0.03819 -27.264  &lt; 2e-16 ***
## incorrect_item:6       -0.64357    0.03548 -18.137  &lt; 2e-16 ***
## incorrect_item:7       -0.15140    0.03366  -4.498 6.85e-06 ***
## incorrect_item:8        0.39278    0.03342  11.754  &lt; 2e-16 ***
## incorrect_item:9        1.00712    0.03537  28.476  &lt; 2e-16 ***
## incorrect_item:10       1.67619    0.04055  41.336  &lt; 2e-16 ***
## countryDrinkland        0.15288    0.01952   7.833 4.77e-15 ***
## chat_incorrect_item:1   2.32686    0.07962  29.226  &lt; 2e-16 ***
## chat_incorrect_item:2   2.19904    0.06878  31.973  &lt; 2e-16 ***
## chat_incorrect_item:3   2.02606    0.06201  32.675  &lt; 2e-16 ***
## chat_incorrect_item:4   1.77040    0.05747  30.807  &lt; 2e-16 ***
## chat_incorrect_item:5   1.50932    0.05478  27.552  &lt; 2e-16 ***
## chat_incorrect_item:6   1.25937    0.05332  23.620  &lt; 2e-16 ***
## chat_incorrect_item:7   0.94683    0.05280  17.932  &lt; 2e-16 ***
## chat_incorrect_item:8   0.71629    0.05438  13.172  &lt; 2e-16 ***
## chat_incorrect_item:9   0.56332    0.05954   9.462  &lt; 2e-16 ***
## chat_incorrect_item:10  0.46708    0.07030   6.644 3.05e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Number of linear predictors:  10 
## 
## Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2]), 
## logitlink(P[Y&lt;=3]), logitlink(P[Y&lt;=4]), logitlink(P[Y&lt;=5]), logitlink(P[Y&lt;=6]), 
## logitlink(P[Y&lt;=7]), logitlink(P[Y&lt;=8]), logitlink(P[Y&lt;=9]), logitlink(P[Y&lt;=10])
## 
## Residual deviance: 17388117 on 399973 degrees of freedom
## 
## Log-likelihood: NA on 399973 degrees of freedom
## 
## Number of Fisher scoring iterations: 2 
## 
## Warning: Hauck-Donner effect detected in the following estimate(s):
## &#39;chat&#39;, &#39;incorrect_item:1&#39;</code></pre>
<p>This model still did not fit properly and produced warnings
indicating unstable parameter estimates. Next, we simplify further by
assuming non-proportional odds only for the interaction factor incorrect
items attended by chat:</p>
<pre class="r"><code># Model where only the interaction is assumed to violate proportional odds
ordinal.model.int.non_proportional3 &lt;- vglm(
  nps.score ~ web_form + chat + email + status + cancellation + incorrect_item + country + chat_incorrect_item,
  family = cumulative(link = logitlink, parallel = ~ country + web_form + chat + email + status + cancellation + incorrect_item),
  data = data
)</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in eval(slot(family, &quot;deriv&quot;)): some probabilities are very close to 0</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1
## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals =
## residuals, : fitted values close to 0 or 1</code></pre>
<pre><code>## Warning in slot(family, &quot;validparams&quot;)(eta, y, extra = extra): It seems that
## the nonparallelism assumption has resulted in intersecting linear/additive
## predictors. Try propodds() or fitting a partial nonproportional odds model or
## choosing some other link function, etc.</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, :
## iterations terminated because half-step sizes are very small</code></pre>
<pre><code>## Warning in vglm.fitter(x = x, y = y, w = w, offset = offset, Xm2 = Xm2, : some
## quantities such as z, residuals, SEs may be inaccurate due to convergence at a
## half-step</code></pre>
<pre class="r"><code>summary(ordinal.model.int.non_proportional3)</code></pre>
<pre><code>## Warning in eval(expr): some probabilities are very close to 0</code></pre>
<pre><code>## 
## Call:
## vglm(formula = nps.score ~ web_form + chat + email + status + 
##     cancellation + incorrect_item + country + chat_incorrect_item, 
##     family = cumulative(link = logitlink, parallel = ~country + 
##         web_form + chat + email + status + cancellation + incorrect_item), 
##     data = data)
## 
## Coefficients: 
##                        Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -0.42189    0.02908 -14.506  &lt; 2e-16 ***
## web_form                0.19959    0.03854   5.178 2.24e-07 ***
## chat                    0.24345    0.02991   8.138 4.00e-16 ***
## email                   0.49162    0.03392  14.493  &lt; 2e-16 ***
## status                 -0.03397    0.02866  -1.185  0.23584    
## cancellation            0.21551    0.03657   5.892 3.80e-09 ***
## incorrect_item          0.01861    0.03617   0.514  0.60691    
## countryDrinkland        0.15288    0.02130   7.176 7.18e-13 ***
## chat_incorrect_item:1  -0.48990    0.05311  -9.224  &lt; 2e-16 ***
## chat_incorrect_item:2  -0.13960    0.05227  -2.671  0.00757 ** 
## chat_incorrect_item:3   0.12318    0.05213   2.363  0.01813 *  
## chat_incorrect_item:4   0.30451    0.05227   5.826 5.69e-09 ***
## chat_incorrect_item:5   0.44946    0.05252   8.557  &lt; 2e-16 ***
## chat_incorrect_item:6   0.59719    0.05292  11.285  &lt; 2e-16 ***
## chat_incorrect_item:7   0.77682    0.05359  14.497  &lt; 2e-16 ***
## chat_incorrect_item:8   1.09046    0.05529  19.722  &lt; 2e-16 ***
## chat_incorrect_item:9   1.55183    0.05925  26.193  &lt; 2e-16 ***
## chat_incorrect_item:10  2.12466    0.06719  31.624  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Number of linear predictors:  10 
## 
## Names of linear predictors: logitlink(P[Y&lt;=1]), logitlink(P[Y&lt;=2]), 
## logitlink(P[Y&lt;=3]), logitlink(P[Y&lt;=4]), logitlink(P[Y&lt;=5]), logitlink(P[Y&lt;=6]), 
## logitlink(P[Y&lt;=7]), logitlink(P[Y&lt;=8]), logitlink(P[Y&lt;=9]), logitlink(P[Y&lt;=10])
## 
## Residual deviance: 19996065 on 399982 degrees of freedom
## 
## Log-likelihood: NA on 399982 degrees of freedom
## 
## Number of Fisher scoring iterations: 2 
## 
## Warning: Hauck-Donner effect detected in the following estimate(s):
## &#39;chat&#39;, &#39;email&#39;</code></pre>
<p>This model still did not fit adequately, suggesting that our data may
not conform well to the non-proportional odds assumptions.</p>
<p>Given this, we explore whether grouping categories of the NPS scale
and focusing on predicting one specific group might help resolve this
issue.</p>
</div>
<div
id="identifying-sections-in-the-nps-scale-with-common-non-proportional-odds"
class="section level6">
<h6>4.3.3.2.3. Identifying Sections in the NPS Scale with Common
Non-Proportional Odds</h6>
<p>To better understand how non-proportional odds manifest across the
NPS scale, we visualize jitter and boxplots using the ggplot2
package:</p>
<pre class="r"><code># Reusable function for combining jitter plots and boxplots in dicotomous factors
f.nps.jitter_box &lt;- function(my.data, my.factor, my.dv) {
  ggplot(my.data, aes(x = factor(.data[[my.factor]]), y = as.numeric(as.character(.data[[my.dv]])))) +
    geom_jitter(width = 0.4, alpha = 0.1, color = &quot;blue&quot;) +  # Adjusted width and alpha for clarity
    geom_boxplot(alpha = 0, outlier.shape = NA) +  # Transparent boxplots to show quartiles
    labs(title = paste(&quot;NPS Score by&quot;, my.factor),
         y = my.dv,
         x = my.factor) +
    theme_minimal() +
    scale_y_continuous(breaks = 0:10) +  # Show all NPS scores on y-axis
    scale_x_discrete(labels = c(&quot;No&quot;, &quot;Yes&quot;))
}

# Apply the function to relevant factors
f.nps.jitter_box(data, &quot;chat&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<pre class="r"><code>f.nps.jitter_box(data, &quot;email&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<pre class="r"><code>f.nps.jitter_box(data, &quot;status&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-3.png" width="672" /></p>
<pre class="r"><code>f.nps.jitter_box(data, &quot;cancellation&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-4.png" width="672" /></p>
<pre class="r"><code>f.nps.jitter_box(data, &quot;incorrect_item&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-5.png" width="672" /></p>
<pre class="r"><code>f.nps.jitter_box(data, &quot;chat_incorrect_item&quot;, &quot;nps.score&quot;)</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-28-6.png" width="672" /></p>
<p>The plots show that factors violating the proportional odds
assumption have stronger effects in the lower range of the NPS
scale—that is, differences are more pronounced among the lower quartile
and below. This suggests these factors primarily influence the formation
of detractors.</p>
<p>To address the non-proportional odds issue and focus on the NPS scale
segment where these factors have the greatest impact, we will next
explore a binomial logistic model predicting the presence of detractors
(those whose NPS scores are 6 or below).</p>
</div>
</div>
</div>
</div>
<div id="binomial-logistic-regression-model-to-predict-detractors"
class="section level3">
<h3>4.3.4. Binomial Logistic Regression Model to Predict Detractors</h3>
<p>Given the limitations of the ordinal model due to violated
proportional odds assumptions, we consider a binomial logistic
regression model more appropriate for predicting detractors. This
approach is also more parsimonious and efficient, focusing specifically
on the group of NPS scores most affected by our factors.</p>
<p>Additionally, prior analyses confirmed that the non-collinearity
assumptions are met.</p>
<p>We then proceed to specify and fit our binomial model:</p>
<pre class="r"><code># Step 1: Create a binary variable for detractors
data$detractor &lt;- NA
data$detractor[data$nps.segment == &quot;detractor&quot;] &lt;- 1
data$detractor[data$nps.segment != &quot;detractor&quot;] &lt;- 0

# Step 2: Fit the binomial logistic regression model
binomial.model &lt;- glm(detractor ~ web_form + chat + email + status + cancellation + incorrect_item + country + chat_incorrect_item, data = data, family = &quot;binomial&quot;)

summary(binomial.model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = detractor ~ web_form + chat + email + status + 
##     cancellation + incorrect_item + country + chat_incorrect_item, 
##     family = &quot;binomial&quot;, data = data)
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -0.60896    0.02323 -26.214  &lt; 2e-16 ***
## web_form             0.22084    0.03670   6.017 1.77e-09 ***
## chat                 0.25318    0.02584   9.796  &lt; 2e-16 ***
## email                0.53630    0.03117  17.207  &lt; 2e-16 ***
## status              -0.04188    0.02696  -1.553    0.120    
## cancellation         0.25598    0.03497   7.321 2.46e-13 ***
## incorrect_item       0.00231    0.03410   0.068    0.946    
## countryDrinkland     0.18005    0.02042   8.816  &lt; 2e-16 ***
## chat_incorrect_item  0.97725    0.05296  18.452  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 54989  on 39999  degrees of freedom
## Residual deviance: 53813  on 39991  degrees of freedom
## AIC: 53831
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The results are consistent with those from the ordinal model, but now
with stable coefficients specifically focused on predicting the
likelihood of being a detractor.</p>
<p>However, the model’s parsimony can still be improved. The contact
reason status does not show any statistically significant effect, so we
test whether the model can be simplified by removing this variable:</p>
<pre class="r"><code># Fit the binomial model excluding the non-significant status factor
binomial.model.c &lt;- glm(detractor ~ web_form + chat + email + cancellation + incorrect_item + country + chat_incorrect_item,
                        data = data, family = &quot;binomial&quot;)

summary(binomial.model.c)</code></pre>
<pre><code>## 
## Call:
## glm(formula = detractor ~ web_form + chat + email + cancellation + 
##     incorrect_item + country + chat_incorrect_item, family = &quot;binomial&quot;, 
##     data = data)
## 
## Coefficients:
##                     Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -0.62103    0.02190 -28.351  &lt; 2e-16 ***
## web_form             0.22079    0.03670   6.016 1.78e-09 ***
## chat                 0.25314    0.02584   9.795  &lt; 2e-16 ***
## email                0.53627    0.03117  17.206  &lt; 2e-16 ***
## cancellation         0.26797    0.03411   7.857 3.94e-15 ***
## incorrect_item       0.01429    0.03322   0.430    0.667    
## countryDrinkland     0.18025    0.02042   8.826  &lt; 2e-16 ***
## chat_incorrect_item  0.97729    0.05296  18.453  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 54989  on 39999  degrees of freedom
## Residual deviance: 53815  on 39992  degrees of freedom
## AIC: 53831
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code># Compare the models with and without the status factor
anova(binomial.model.c, binomial.model, test = &quot;Chisq&quot;)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[""],"name":["_rn_"],"type":[""],"align":["left"]},{"label":["Resid. Df"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["Resid. Dev"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Df"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Deviance"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Pr(>Chi)"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"39992","2":"53815.34","3":"NA","4":"NA","5":"NA","_rn_":"1"},{"1":"39991","2":"53812.93","3":"1","4":"2.415119","5":"0.1201688","_rn_":"2"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>The likelihood ratio test confirms that removing the status variable
does not significantly reduce model fit. Therefore, we simplify the
model by excluding it. This allows us to focus on the most relevant and
actionable predictors of detractor status.</p>
<p>We are now ready to derive meaningful metrics from the model to
understand the extent to which the presence of detractors is associated
with specific customer service factors.</p>
<div id="creating-metrics-to-interpret-the-model"
class="section level4">
<h4>4.3.4.1. Creating Metrics to Interpret the Model</h4>
<p>To interpret the results of our binomial logistic regression, we will
compute several complementary metrics for each of the factors:</p>
<ul>
<li><p><strong>Predicted percentage of detractors</strong> for the
presence of each customer service factor (while holding others
constant).</p></li>
<li><p><strong>Differences in predicted percentages</strong> compared to
the baseline.</p></li>
<li><p><strong>Relative probabilities</strong> (risk ratios).</p></li>
<li><p><strong>Odds ratios</strong>.</p></li>
</ul>
<p>To calculate the predicted proportions, we first need to build a
design matrix that represents the isolated presence of each factor (and
the intercept) in a way compatible with our model specification. This
will allow us to generate predictions for each scenario separately.</p>
<pre class="r"><code>#####
# Design Matrix for Predictions
#####

# Extract the factor names from the model, with and without the intercept
f.names.interc &lt;- names(coef(binomial.model.c))
f.names &lt;- f.names.interc[-1]

# Create a matrix with 1s on the diagonal (each case represents one factor set to 1)
my.matrix.df &lt;- as.data.frame(diag(length(f.names.interc)))
my.matrix.df[, 1] &lt;- f.names.interc
names(my.matrix.df) &lt;- c(&quot;case&quot;, f.names)

# Correct the interaction term so its components are also activated
my.matrix.df$chat[my.matrix.df$case == &quot;chat_incorrect_item&quot;] &lt;- 1
my.matrix.df$incorrect_item[my.matrix.df$case == &quot;chat_incorrect_item&quot;] &lt;- 1

# Ensure factor variables match the dataset format: we need to do it for country. 
names(my.matrix.df)[names(my.matrix.df) == &quot;countryDrinkland&quot;] &lt;- &quot;country&quot;
my.matrix.df$country[my.matrix.df$country == 1] &lt;- &quot;Drinkland&quot;
my.matrix.df$country[my.matrix.df$country == 0] &lt;- &quot;Eatland&quot;
my.matrix.df$country &lt;- as.factor(my.matrix.df$country)

# Quick check of the structure
str(my.matrix.df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    8 obs. of  8 variables:
##  $ case               : chr  &quot;(Intercept)&quot; &quot;web_form&quot; &quot;chat&quot; &quot;email&quot; ...
##  $ web_form           : num  0 1 0 0 0 0 0 0
##  $ chat               : num  0 0 1 0 0 0 0 1
##  $ email              : num  0 0 0 1 0 0 0 0
##  $ cancellation       : num  0 0 0 0 1 0 0 0
##  $ incorrect_item     : num  0 0 0 0 0 1 0 1
##  $ country            : Factor w/ 2 levels &quot;Drinkland&quot;,&quot;Eatland&quot;: 2 2 2 2 2 2 1 2
##  $ chat_incorrect_item: num  0 0 0 0 0 0 0 1</code></pre>
<p>We then create a function to compute the metrics and their 95%
confidence intervals. Confidence intervals for differences in predicted
proportions are estimated via parametric bootstrap simulations</p>
<pre class="r"><code>#####
#Function to obtain key interpretation metrics for a binomial logistic model
#####


f.metrics.binomial &lt;- function(my.model, my.matrix){
  
  #--------1. Preliminary Checks: 
  
  # Making sure the model is glm binomial
  if (!inherits(my.model, &quot;glm&quot;) || my.model$family$family != &quot;binomial&quot;) {
    stop(&quot;The function requires a binomial model.&quot;)
  }
  
  
  #--------2. Odds Ratios
  
  # Obtaining the coefficients and confidence intervals
  coefs &lt;- coef(my.model)
  ci &lt;- confint(my.model)
  intercept &lt;- coefs[1]
  
  # Exponentiation of the coefficients and their CIs to obtain odds ratios
  or &lt;- exp(coefs)
  or_ci &lt;- exp(ci)
  
  
  #--------3. Relative Probabilities (Risk Ratios)
  
  # Calculating the base probability from the intercept
  p0 &lt;- or[1] / (1 + or[1])
  
  # Obtaining relative probabilities from risk ratios and base probability
  rp &lt;- or / ((1 - p0) + (p0 * or))
  rp_ci_low &lt;- or_ci[, 1] / ((1 - p0) + (p0 * or_ci[, 1]))
  rp_ci_high &lt;- or_ci[, 2] / ((1 - p0) + (p0 * or_ci[, 2]))
  
  
  #--------4. Predicted Proportions
  
  # Extract log-odds and their sd for the individual presence of the factors (using the specified matrix)
  pred &lt;- predict(my.model, newdata = my.matrix, type = &quot;link&quot;, se.fit = TRUE)

  #Calculate confidence intervals in log-odds
  lower_logit &lt;- pred$fit - 1.96 * pred$se.fit
  upper_logit &lt;- pred$fit + 1.96 * pred$se.fit

  #Specify a function to transform log-odds to probabilities
  f.inv_logit &lt;- function(x) {1 / (1 + exp(-x))}
  
  #Apply the function to the log-odds and the limits of their ci
  predicted_prob &lt;- f.inv_logit(pred$fit) # Predicted probabilities
  lower_predicted_prob &lt;- f.inv_logit(lower_logit) # Lower limit of CI for predicted probabilities
  upper_predicted_prob &lt;- f.inv_logit(upper_logit) # Upper limit of CI for predicted probabilities

  
  #--------5. Differences in Predicted Probabilities via Bootstrap

  # Simulate a distribution of log-odds, for example with 10000 cases
  set.seed(123)
  simulated_log_odds &lt;- as.data.frame(matrix(
  rnorm(10000 * length(pred$fit), mean = pred$fit, sd = pred$se.fit),
  ncol = length(pred$fit), byrow = TRUE
  ))

  # Transform logg-odds to expected probabilities (using the formula specified above)
  simulated_probs &lt;- f.inv_logit(simulated_log_odds)

  # Calculate differences from the first column, which correspond to the intercept probabilities
  diffs &lt;- apply(simulated_probs, 2, function(col){
  col - simulated_probs[[1]]
  })

  # Calculate the means of the differences and 95% CI with percentiles
  prob_diff &lt;- colMeans(diffs)
  lower_prob_diff  &lt;- apply(diffs, 2, quantile, probs = 0.025)
  upper_prob_diff &lt;- apply(diffs, 2, quantile, probs = 0.975)
  
  
  #-------- 6. Combine Results
  
  results &lt;- data.frame(
    Term = names(rp),
    Prediction = as.numeric(predicted_prob)*100,
    Lower_Prediction = as.numeric(lower_predicted_prob)*100,
    Upper_Prediction = as.numeric(upper_predicted_prob)*100,
    Predicted_Change = as.numeric(prob_diff)*100,
    Lower_Predicted_Change = as.numeric(lower_prob_diff)*100,
    Upper_Predicted_Change = as.numeric(upper_prob_diff)*100,
    Relative_Probability = as.numeric(rp),
    Lower_Relative_Probability = as.numeric(rp_ci_low),
    Upper_Relative_Probability = as.numeric(rp_ci_high),
    Odds_Ratio = as.numeric(or),
    Lower_Odds_Ratio = as.numeric(or_ci[,1]),
    Upper_Odds_Ratio = as.numeric(or_ci[,2])
)
  
  # Remove meaningless metrics for intercept
  results[1, c(&quot;Predicted_Change&quot;,
             &quot;Lower_Predicted_Change&quot;,
             &quot;Upper_Predicted_Change&quot;,
             &quot;Relative_Probability&quot;,
             &quot;Lower_Relative_Probability&quot;,
             &quot;Upper_Relative_Probability&quot;,
             &quot;Odds_Ratio&quot;,
             &quot;Lower_Odds_Ratio&quot;,
             &quot;Upper_Odds_Ratio&quot;)] &lt;- NA_real_

  
  # Return the results
  return(results)
}</code></pre>
<p>Now we can apply the function to our final binomial model in order to
generate the set of metrics described above.</p>
<pre class="r"><code># Apply the function to the final binomial model
model.metrics.df &lt;- f.metrics.binomial(binomial.model.c, my.matrix.df)</code></pre>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre class="r"><code># Display the resulting metrics
model.metrics.df</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Prediction"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Lower_Prediction"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Upper_Prediction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Predicted_Change"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Lower_Predicted_Change"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Upper_Predicted_Change"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["Relative_Probability"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["Lower_Relative_Probability"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Upper_Relative_Probability"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["Odds_Ratio"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["Lower_Odds_Ratio"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["Upper_Odds_Ratio"],"name":[13],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"34.95480","3":"33.98506","4":"35.93715","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA"},{"1":"web_form","2":"40.12560","3":"38.50178","4":"41.77138","5":"5.1654411","6":"3.286841","7":"7.068182","8":"1.147928","9":"1.0988167","10":"1.197658","11":"1.247064","12":"1.1604400","13":"1.339992"},{"1":"chat","2":"40.90509","3":"39.90661","4":"41.91113","5":"5.9443321","6":"4.541586","7":"7.331843","8":"1.170228","9":"1.1353773","10":"1.205422","11":"1.288059","12":"1.2244574","13":"1.355003"},{"1":"email","2":"47.88224","3":"46.47219","4":"49.29567","5":"12.9220210","6":"11.203495","7":"14.649027","8":"1.369833","9":"1.3263056","10":"1.413502","11":"1.709610","12":"1.6083338","13":"1.817337"},{"1":"cancellation","2":"41.26410","3":"39.53295","4":"43.01712","5":"6.2982647","6":"4.347519","7":"8.265228","8":"1.180499","9":"1.1344224","10":"1.227097","11":"1.307305","12":"1.2227509","13":"1.397669"},{"1":"incorrect_item","2":"35.28039","3":"33.81002","4":"36.77916","5":"0.3301661","6":"-1.428853","7":"2.169299","8":"1.009314","9":"0.9671599","10":"1.052196","11":"1.014392","12":"0.9503875","13":"1.082562"},{"1":"countryDrinkland","2":"39.15569","3":"38.15204","4":"40.16859","5":"4.1972629","6":"2.803931","7":"5.607530","8":"1.120180","9":"1.0930251","10":"1.147582","11":"1.197521","12":"1.1505418","13":"1.246436"},{"1":"chat_incorrect_item","2":"65.10563","3":"63.36170","4":"66.81003","5":"30.1460325","6":"28.144926","7":"32.142751","8":"1.682558","9":"1.6101524","10":"1.753902","11":"2.657235","12":"2.3956728","13":"2.948434"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To facilitate interpretation, we rearrange the results in descending
order of the predicted percentage change, keeping the intercept at the
top of the table:</p>
<pre class="r"><code># Arrange results by predicted percentage change, keeping the intercept at the top
int.df &lt;- model.metrics.df[1, ]
factors.df &lt;- arrange(model.metrics.df[-1, ], desc(Predicted_Change))
ord.model.metrics.df &lt;- bind_rows(int.df, factors.df)

# Display the reordered results
ord.model.metrics.df</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Prediction"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Lower_Prediction"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Upper_Prediction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Predicted_Change"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Lower_Predicted_Change"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Upper_Predicted_Change"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["Relative_Probability"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["Lower_Relative_Probability"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Upper_Relative_Probability"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["Odds_Ratio"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["Lower_Odds_Ratio"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["Upper_Odds_Ratio"],"name":[13],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"34.95480","3":"33.98506","4":"35.93715","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA"},{"1":"chat_incorrect_item","2":"65.10563","3":"63.36170","4":"66.81003","5":"30.1460325","6":"28.144926","7":"32.142751","8":"1.682558","9":"1.6101524","10":"1.753902","11":"2.657235","12":"2.3956728","13":"2.948434"},{"1":"email","2":"47.88224","3":"46.47219","4":"49.29567","5":"12.9220210","6":"11.203495","7":"14.649027","8":"1.369833","9":"1.3263056","10":"1.413502","11":"1.709610","12":"1.6083338","13":"1.817337"},{"1":"cancellation","2":"41.26410","3":"39.53295","4":"43.01712","5":"6.2982647","6":"4.347519","7":"8.265228","8":"1.180499","9":"1.1344224","10":"1.227097","11":"1.307305","12":"1.2227509","13":"1.397669"},{"1":"chat","2":"40.90509","3":"39.90661","4":"41.91113","5":"5.9443321","6":"4.541586","7":"7.331843","8":"1.170228","9":"1.1353773","10":"1.205422","11":"1.288059","12":"1.2244574","13":"1.355003"},{"1":"web_form","2":"40.12560","3":"38.50178","4":"41.77138","5":"5.1654411","6":"3.286841","7":"7.068182","8":"1.147928","9":"1.0988167","10":"1.197658","11":"1.247064","12":"1.1604400","13":"1.339992"},{"1":"countryDrinkland","2":"39.15569","3":"38.15204","4":"40.16859","5":"4.1972629","6":"2.803931","7":"5.607530","8":"1.120180","9":"1.0930251","10":"1.147582","11":"1.197521","12":"1.1505418","13":"1.246436"},{"1":"incorrect_item","2":"35.28039","3":"33.81002","4":"36.77916","5":"0.3301661","6":"-1.428853","7":"2.169299","8":"1.009314","9":"0.9671599","10":"1.052196","11":"1.014392","12":"0.9503875","13":"1.082562"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>We can further enhance interpretability by adding clear,
reader-oriented labels for each factor in the results table:</p>
<pre class="r"><code># Add descriptive labels for each factor
ord.model.metrics.df$Label &lt;- NA_character_
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;(Intercept)&quot;] &lt;- 
  &quot;Baseline: Typical issues handled via phone in Eatland&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;chat_incorrect_item&quot;] &lt;- 
  &quot;Chat support for incorrect item issues&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;email&quot;] &lt;- 
  &quot;Support via email&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;chat&quot;] &lt;- 
  &quot;Support via chat&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;web_form&quot;] &lt;- 
  &quot;Support via web form&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;countryDrinkland&quot;] &lt;- 
  &quot;Support in Drinkland&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;cancellation&quot;] &lt;- 
  &quot;Support for cancellation issues&quot;
ord.model.metrics.df$Label[ord.model.metrics.df$Term == &quot;incorrect_item&quot;] &lt;- 
  &quot;Support for incorrect item issues&quot;

# Quick check to ensure labels are as expected
ord.model.metrics.df</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Prediction"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Lower_Prediction"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Upper_Prediction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Predicted_Change"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Lower_Predicted_Change"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Upper_Predicted_Change"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["Relative_Probability"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["Lower_Relative_Probability"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Upper_Relative_Probability"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["Odds_Ratio"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["Lower_Odds_Ratio"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["Upper_Odds_Ratio"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["Label"],"name":[14],"type":["chr"],"align":["left"]}],"data":[{"1":"(Intercept)","2":"34.95480","3":"33.98506","4":"35.93715","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA","14":"Baseline: Typical issues handled via phone in Eatland"},{"1":"chat_incorrect_item","2":"65.10563","3":"63.36170","4":"66.81003","5":"30.1460325","6":"28.144926","7":"32.142751","8":"1.682558","9":"1.6101524","10":"1.753902","11":"2.657235","12":"2.3956728","13":"2.948434","14":"Chat support for incorrect item issues"},{"1":"email","2":"47.88224","3":"46.47219","4":"49.29567","5":"12.9220210","6":"11.203495","7":"14.649027","8":"1.369833","9":"1.3263056","10":"1.413502","11":"1.709610","12":"1.6083338","13":"1.817337","14":"Support via email"},{"1":"cancellation","2":"41.26410","3":"39.53295","4":"43.01712","5":"6.2982647","6":"4.347519","7":"8.265228","8":"1.180499","9":"1.1344224","10":"1.227097","11":"1.307305","12":"1.2227509","13":"1.397669","14":"Support for cancellation issues"},{"1":"chat","2":"40.90509","3":"39.90661","4":"41.91113","5":"5.9443321","6":"4.541586","7":"7.331843","8":"1.170228","9":"1.1353773","10":"1.205422","11":"1.288059","12":"1.2244574","13":"1.355003","14":"Support via chat"},{"1":"web_form","2":"40.12560","3":"38.50178","4":"41.77138","5":"5.1654411","6":"3.286841","7":"7.068182","8":"1.147928","9":"1.0988167","10":"1.197658","11":"1.247064","12":"1.1604400","13":"1.339992","14":"Support via web form"},{"1":"countryDrinkland","2":"39.15569","3":"38.15204","4":"40.16859","5":"4.1972629","6":"2.803931","7":"5.607530","8":"1.120180","9":"1.0930251","10":"1.147582","11":"1.197521","12":"1.1505418","13":"1.246436","14":"Support in Drinkland"},{"1":"incorrect_item","2":"35.28039","3":"33.81002","4":"36.77916","5":"0.3301661","6":"-1.428853","7":"2.169299","8":"1.009314","9":"0.9671599","10":"1.052196","11":"1.014392","12":"0.9503875","13":"1.082562","14":"Support for incorrect item issues"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Lastly, we visually represent the predicted proportions of detractors
to clearly identify which key customer service factors are associated
with higher likelihoods of detractor status.</p>
<pre class="r"><code># Create the dot-and-whisker plot for the predictions
ggplot(ord.model.metrics.df, aes(x = Prediction, 
                    y = reorder(Label, Prediction))) +
  geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;darkgrey&quot;) +
  geom_errorbarh(aes(xmin = Lower_Prediction, xmax = Upper_Prediction), height = 0.2) +
  geom_point(size = 3) +
    geom_text(aes(label = paste(round(Prediction,1), &quot;%&quot;)), 
            vjust = -1, size = 3.5) + 
  labs(
    title = &quot;Expected Detractors By Key Customer Service Factors&quot;,
    x = &quot;% of Detractors&quot;,
    y = &quot;Customer Service Factors&quot;
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;))</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>The graph shows that contacting customer service through channels
other than telephone is linked to a higher proportion of
detractors—especially via email, or via chat when addressing incorrect
item issues.</p>
<p>Additionally, customer service interactions in Drinkland correspond
to more detractors compared to Eatland.</p>
<p>Finally, customers reaching out about cancellation issues also tend
to have more detractors than those contacting for typical issues.</p>
<p>It is important to note, however, that the overall impact of these
factors depends on the volume of customers contacting us for each
reason. We will account for this in the next section.</p>
</div>
</div>
</div>
<div id="evaluating-the-impact-of-contextual-customer-service-factors"
class="section level2">
<h2>4.4. Evaluating the Impact of Contextual Customer Service
Factors</h2>
<p>To assess the impact in terms of additional detractors associated
with each customer service factor, we weight the predicted proportions
of detractors by the proportion of cases associated with each
factor.</p>
<p>We assume the following distribution of cases across the factors:</p>
<ul>
<li>Baseline: Typical issues handled via phone in Eatland — 10%</li>
<li>Attention via chat for incorrect item issues — 10%</li>
<li>Attention via email — 20%</li>
<li>Attention via chat — 40%</li>
<li>Attention via web form — 5%</li>
<li>Attention in the country Drinkland — 50%</li>
<li>Attention for cancellation issues — 5%</li>
<li>Attention for incorrect item issues — 30%</li>
</ul>
<p>Using these proportions, we create impact variables by weighting both
the predicted percentages of detractors and their differences across
factors:</p>
<pre class="r"><code># Create a variable specifying the proportion of cases
ord.model.metrics.df$prop.cases &lt;- c(.10, .10, .20, .40, .05, .50, .05, .30)

# Define variables to weight
variables_to_mutate &lt;- c(
  &quot;Prediction&quot;,
  &quot;Lower_Prediction&quot;,
  &quot;Upper_Prediction&quot;,
  &quot;Predicted_Change&quot;,
  &quot;Lower_Predicted_Change&quot;,
  &quot;Upper_Predicted_Change&quot;
)

# Calculate weighted impact variables
for(variable in variables_to_mutate) {
  new_col &lt;- paste0(&quot;Impact_&quot;, variable)
  ord.model.metrics.df[[new_col]] &lt;- ord.model.metrics.df[[&quot;prop.cases&quot;]] * ord.model.metrics.df[[variable]]
}

# Review the updated dataframe
ord.model.metrics.df</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["Term"],"name":[1],"type":["chr"],"align":["left"]},{"label":["Prediction"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["Lower_Prediction"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["Upper_Prediction"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["Predicted_Change"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["Lower_Predicted_Change"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["Upper_Predicted_Change"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["Relative_Probability"],"name":[8],"type":["dbl"],"align":["right"]},{"label":["Lower_Relative_Probability"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["Upper_Relative_Probability"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["Odds_Ratio"],"name":[11],"type":["dbl"],"align":["right"]},{"label":["Lower_Odds_Ratio"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["Upper_Odds_Ratio"],"name":[13],"type":["dbl"],"align":["right"]},{"label":["Label"],"name":[14],"type":["chr"],"align":["left"]},{"label":["prop.cases"],"name":[15],"type":["dbl"],"align":["right"]},{"label":["Impact_Prediction"],"name":[16],"type":["dbl"],"align":["right"]},{"label":["Impact_Lower_Prediction"],"name":[17],"type":["dbl"],"align":["right"]},{"label":["Impact_Upper_Prediction"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["Impact_Predicted_Change"],"name":[19],"type":["dbl"],"align":["right"]},{"label":["Impact_Lower_Predicted_Change"],"name":[20],"type":["dbl"],"align":["right"]},{"label":["Impact_Upper_Predicted_Change"],"name":[21],"type":["dbl"],"align":["right"]}],"data":[{"1":"(Intercept)","2":"34.95480","3":"33.98506","4":"35.93715","5":"NA","6":"NA","7":"NA","8":"NA","9":"NA","10":"NA","11":"NA","12":"NA","13":"NA","14":"Baseline: Typical issues handled via phone in Eatland","15":"0.10","16":"3.495480","17":"3.398506","18":"3.593715","19":"NA","20":"NA","21":"NA"},{"1":"chat_incorrect_item","2":"65.10563","3":"63.36170","4":"66.81003","5":"30.1460325","6":"28.144926","7":"32.142751","8":"1.682558","9":"1.6101524","10":"1.753902","11":"2.657235","12":"2.3956728","13":"2.948434","14":"Chat support for incorrect item issues","15":"0.10","16":"6.510563","17":"6.336170","18":"6.681003","19":"3.01460325","20":"2.8144926","21":"3.2142751"},{"1":"email","2":"47.88224","3":"46.47219","4":"49.29567","5":"12.9220210","6":"11.203495","7":"14.649027","8":"1.369833","9":"1.3263056","10":"1.413502","11":"1.709610","12":"1.6083338","13":"1.817337","14":"Support via email","15":"0.20","16":"9.576447","17":"9.294437","18":"9.859134","19":"2.58440420","20":"2.2406990","21":"2.9298054"},{"1":"cancellation","2":"41.26410","3":"39.53295","4":"43.01712","5":"6.2982647","6":"4.347519","7":"8.265228","8":"1.180499","9":"1.1344224","10":"1.227097","11":"1.307305","12":"1.2227509","13":"1.397669","14":"Support for cancellation issues","15":"0.40","16":"16.505638","17":"15.813180","18":"17.206847","19":"2.51930586","20":"1.7390077","21":"3.3060911"},{"1":"chat","2":"40.90509","3":"39.90661","4":"41.91113","5":"5.9443321","6":"4.541586","7":"7.331843","8":"1.170228","9":"1.1353773","10":"1.205422","11":"1.288059","12":"1.2244574","13":"1.355003","14":"Support via chat","15":"0.05","16":"2.045255","17":"1.995331","18":"2.095557","19":"0.29721660","20":"0.2270793","21":"0.3665921"},{"1":"web_form","2":"40.12560","3":"38.50178","4":"41.77138","5":"5.1654411","6":"3.286841","7":"7.068182","8":"1.147928","9":"1.0988167","10":"1.197658","11":"1.247064","12":"1.1604400","13":"1.339992","14":"Support via web form","15":"0.50","16":"20.062798","17":"19.250892","18":"20.885688","19":"2.58272054","20":"1.6434204","21":"3.5340910"},{"1":"countryDrinkland","2":"39.15569","3":"38.15204","4":"40.16859","5":"4.1972629","6":"2.803931","7":"5.607530","8":"1.120180","9":"1.0930251","10":"1.147582","11":"1.197521","12":"1.1505418","13":"1.246436","14":"Support in Drinkland","15":"0.05","16":"1.957784","17":"1.907602","18":"2.008429","19":"0.20986315","20":"0.1401966","21":"0.2803765"},{"1":"incorrect_item","2":"35.28039","3":"33.81002","4":"36.77916","5":"0.3301661","6":"-1.428853","7":"2.169299","8":"1.009314","9":"0.9671599","10":"1.052196","11":"1.014392","12":"0.9503875","13":"1.082562","14":"Support for incorrect item issues","15":"0.30","16":"10.584116","17":"10.143007","18":"11.033749","19":"0.09904984","20":"-0.4286560","21":"0.6507896"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>To better understand the relative impact of each customer service
factor on the increase in detractors, we visualize the additional
predicted detractors associated with each factor.</p>
<p>The following dot-and-whisker plot displays the estimated percentage
increase in detractors, adjusted by the proportion of cases for each
factor. Error bars represent the 95% confidence intervals.</p>
<pre class="r"><code># Create the dot-and-whisker plot for the additional detractors associated to each case.
ggplot(ord.model.metrics.df[-1, ], aes(x = Impact_Predicted_Change, 
                    y = reorder(Label, Impact_Predicted_Change))) +
  geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;darkgrey&quot;) +
  geom_errorbarh(aes(xmin = Impact_Lower_Predicted_Change, xmax = Impact_Upper_Predicted_Change), height = 0.2) +
  geom_point(size = 3) +
  geom_text(aes(label = paste0(round(Impact_Predicted_Change, 1), &quot;%&quot;)), 
            vjust = -1.5, size = 3.5) + 
  labs(
    title = &quot;Impact of Key Customer Service Factors&quot;,
    x = &quot;% of Additional Detractors Compared to Baseline&quot;,
    y = &quot;Customer Service Factors&quot;
  ) +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5, face = &quot;bold&quot;))</code></pre>
<p><img src="Predicting-Customer-Detractors--Part-1-.-Analyzing-Contextual-Factors-Via-Logistic-Regression_files/figure-html/unnamed-chunk-38-1.png" width="672" /></p>
<p>This graph allows us to visualize the absolute percentage of
detractors expected under each key contextual customer service factor,
compared to the baseline scenario (support via phone in Eatland for
typical issues).</p>
<p>For example, we can communicate to stakeholders that if we
successfully address problems experienced by customers contacting via
chat for incorrect item issues—bringing the detractor probability down
to the baseline level—we could expect an approximate 3% reduction in
overall detractors related to customer service.</p>
<p>However, we do not yet know the specific issues customers encounter
within each contextual factor. This will be explored in the next study
case: <strong>“Predicting Customer Detractors (Part 2): Assessing
Improvement Opportunities via Text Analysis”</strong></p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
